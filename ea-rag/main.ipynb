{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `.env` file with the following details:\n",
    "- `OPENAI_API_KEY`\n",
    "- `PINECONE_API_KEY`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation triples\n",
    "Convert tabular data in `en_rel_triples` and `fr_rel_triples` to N-Triples format. Example:\n",
    "- input: `http://dbpedia.org/resource/Virton\thttp://dbpedia.org/property/nw\thttp://dbpedia.org/resource/Tintigny`\n",
    "- output: `<http://dbpedia.org/resource/Virton>\t<http://dbpedia.org/property/nw>\t<http://dbpedia.org/resource/Tintigny> .`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-Triples conversion complete for 'fr_en/en_rel_triples'. Output saved to 'fr_en/en_rel_triples_preprocessed'.\n",
      "N-Triples conversion complete for 'fr_en/fr_rel_triples'. Output saved to 'fr_en/fr_rel_triples_preprocessed'.\n"
     ]
    }
   ],
   "source": [
    "def convert_to_ntriples(input_file_path, output_file_path):\n",
    "    \"\"\"\n",
    "    Converts a TSV RDF dataset to N-Triples format.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_file_path (str): Path to the input TSV file.\n",
    "    - output_file_path (str): Path to save the processed N-Triples file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(input_file_path, \"r\", encoding=\"utf-8\") as infile, open(output_file_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "            for line in infile:\n",
    "                # Split each line using tab as the delimiter\n",
    "                parts = line.strip().split(\"\\t\")\n",
    "                \n",
    "                # Ensure the line has exactly 3 parts\n",
    "                if len(parts) == 3:\n",
    "                    subject, predicate, obj = parts\n",
    "                    # Format the line according to the N-Triples format\n",
    "                    n_triple_line = f\"<{subject}> <{predicate}> <{obj}> .\\n\"\n",
    "                    outfile.write(n_triple_line)\n",
    "                else:\n",
    "                    print(f\"Skipping invalid line: {line.strip()}\")\n",
    "\n",
    "        print(f\"N-Triples conversion complete for '{input_file_path}'. Output saved to '{output_file_path}'.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{input_file_path}' was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# List of datasets to process\n",
    "datasets = [\n",
    "    (\"fr_en/en_rel_triples\", \"fr_en/en_rel_triples_preprocessed\"),\n",
    "    (\"fr_en/fr_rel_triples\", \"fr_en/fr_rel_triples_preprocessed\")\n",
    "]\n",
    "\n",
    "# Loop through the datasets and process them\n",
    "for input_path, output_path in datasets:\n",
    "    convert_to_ntriples(input_path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attribute triples\n",
    "Converting negative dates to a format that can be processed using Python. Example:\n",
    "- input: `-0043-12-07`\n",
    "- output: `0043-12-07 BCE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed RDF data saved to: fr_en/en_att_triples_preprocessed\n",
      "Preprocessed RDF data saved to: fr_en/fr_att_triples_preprocessed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def preprocess_dates_in_file(input_file_path, output_file_path):\n",
    "    \"\"\"\n",
    "    Preprocesses the RDF file to convert negative xsd:date values to a BCE format as plain strings.\n",
    "\n",
    "    Parameters:\n",
    "    - input_file_path (str): Path to the original RDF file.\n",
    "    - output_file_path (str): Path to save the preprocessed RDF file.\n",
    "    \"\"\"\n",
    "    with open(input_file_path, \"r\", encoding=\"utf-8\") as infile, open(output_file_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        for line in infile:\n",
    "            # Detecting negative dates and converting them to string format\n",
    "            if '^^<http://www.w3.org/2001/XMLSchema#date>' in line:\n",
    "                start_index = line.find('\"') + 1\n",
    "                end_index = line.find('\"', start_index)\n",
    "                \n",
    "                if start_index != -1 and end_index != -1:\n",
    "                    date_string = line[start_index:end_index]\n",
    "                    if date_string.startswith(\"-\"):\n",
    "                        # Convert negative date to BCE string and remove type declaration\n",
    "                        sanitized_date = f'\"{date_string[1:]} BCE\"'\n",
    "                        # Remove the xsd:date type and keep it as a plain string\n",
    "                        line = line[:start_index-1] + sanitized_date + \" .\\n\"  # Added newline character\n",
    "            # Ensure each line ends with a newline character even if unmodified\n",
    "            if not line.endswith(\"\\n\"):\n",
    "                line += \"\\n\"\n",
    "            outfile.write(line)\n",
    "    \n",
    "    print(f\"Preprocessed RDF data saved to: {output_file_path}\")\n",
    "\n",
    "# Preprocess both datasets\n",
    "datasets_to_preprocess = [\n",
    "    (\"fr_en/en_att_triples\", \"fr_en/en_att_triples_preprocessed\"),\n",
    "    (\"fr_en/fr_att_triples\", \"fr_en/fr_att_triples_preprocessed\")\n",
    "]\n",
    "\n",
    "for input_file, output_file in datasets_to_preprocess:\n",
    "    preprocess_dates_in_file(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph creation & description generation\n",
    "Processes the datasets with the following steps:\n",
    "1) Creates a graph comprised of relation and attribute triples;\n",
    "2) Extracts all pieces of information about a node and stores it in a `.txt` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this version is deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged graph created with 855133 triples.\n",
      "Files saved in 'fr_en/en_combined_triples_folder2'.\n",
      "Merged graph created with 720856 triples.\n",
      "Files saved in 'fr_en/fr_combined_triples_folder2'.\n"
     ]
    }
   ],
   "source": [
    "# from rdflib import Graph\n",
    "# import os\n",
    "\n",
    "# def create_merged_graph(relation_file_path, attribute_file_path):\n",
    "#     \"\"\"Creates and returns an RDF graph by merging relation and attribute files.\"\"\"\n",
    "#     graph = Graph()\n",
    "#     graph.parse(relation_file_path, format=\"nt\")\n",
    "#     graph.parse(attribute_file_path, format=\"nt\")\n",
    "\n",
    "#     print(f\"Merged graph created with {len(graph)} triples.\")\n",
    "#     return graph\n",
    "\n",
    "# def format_triples_for_embedding(graph, entity_uri):\n",
    "#     \"\"\"\n",
    "#     Formats RDF triples where the given entity is a subject or object into a readable format for embeddings.\n",
    "\n",
    "#     Parameters:\n",
    "#     - graph: RDFLib Graph object\n",
    "#     - entity_uri: URI of the entity to query for\n",
    "\n",
    "#     Returns:\n",
    "#     - formatted_text: A single string containing all triples where the entity is subject or object, ready for embedding generation.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Extract the entity label from the URI\n",
    "#     entity_label = entity_uri.split('/')[-1]\n",
    "\n",
    "#     def safe_split(uri):\n",
    "#         \"\"\"Replaces None with the current entity label and extracts labels from URIs.\"\"\"\n",
    "#         if uri is None:\n",
    "#             return entity_label\n",
    "#         return uri.split('/')[-1] if '/' in str(uri) else str(uri)\n",
    "\n",
    "#     # Prepare SPARQL queries for both subject and object positions\n",
    "#     query_subject = f\"\"\"\n",
    "#     SELECT ?s ?p ?o\n",
    "#     WHERE {{\n",
    "#         <{entity_uri}> ?p ?o .\n",
    "#     }}\n",
    "#     \"\"\"\n",
    "    \n",
    "#     query_object = f\"\"\"\n",
    "#     SELECT ?s ?p ?o\n",
    "#     WHERE {{\n",
    "#         ?s ?p <{entity_uri}> .\n",
    "#     }}\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Execute the queries\n",
    "#     subject_results = graph.query(query_subject)\n",
    "#     object_results = graph.query(query_object)\n",
    "\n",
    "#     # Prepare the formatted text for embeddings\n",
    "#     formatted_text = []\n",
    "\n",
    "#     # Format triples where the entity is the subject\n",
    "#     formatted_text.append(f\"# Triples where '{entity_label}' is the subject:\\n\")\n",
    "#     for s, p, o in subject_results:\n",
    "#         formatted_text.append(f\"{safe_split(s)} {safe_split(p)} {safe_split(o)}.\")\n",
    "\n",
    "#     # Format triples where the entity is the object\n",
    "#     formatted_text.append(f\"\\n# Triples where '{entity_label}' is the object:\\n\")\n",
    "#     for s, p, o in object_results:\n",
    "#         formatted_text.append(f\"{safe_split(s)} {safe_split(p)} {safe_split(o)}.\")\n",
    "\n",
    "#     # Combine all triples into a single formatted string\n",
    "#     formatted_text = \"\\n\".join(formatted_text)\n",
    "#     return formatted_text\n",
    "\n",
    "# def describe_node_for_embedding_per_subject(graph, output_folder):\n",
    "#     \"\"\"Extracts all attributes and relations for every node and saves them as text files.\"\"\"\n",
    "#     os.makedirs(output_folder, exist_ok=True)\n",
    "#     for subject in set(graph.subjects()):\n",
    "#         subject_label = subject.split('/')[-1]\n",
    "#         subject_file_path = os.path.join(output_folder, f\"{subject_label}.txt\")\n",
    "\n",
    "#         # Use the new function to generate formatted triples for the subject\n",
    "#         formatted_text = format_triples_for_embedding(graph, subject)\n",
    "\n",
    "#         # Write the formatted data to the file\n",
    "#         with open(subject_file_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "#             outfile.write(formatted_text)\n",
    "#     print(f\"Files saved in '{output_folder}'.\")\n",
    "\n",
    "# def process_multiple_datasets(datasets):\n",
    "#     \"\"\"Processes multiple RDF datasets and saves results for each.\"\"\"\n",
    "#     for relation_file, attribute_file, output_folder in datasets:\n",
    "#         graph = create_merged_graph(relation_file, attribute_file)\n",
    "#         describe_node_for_embedding_per_subject(graph, output_folder)\n",
    "\n",
    "# # List of datasets including both relation and attribute triples\n",
    "# datasets_to_process = [\n",
    "#     (\"fr_en/en_rel_triples_preprocessed\", \"fr_en/en_att_triples_preprocessed\", \"fr_en/en_combined_triples_folder2\"),\n",
    "#     (\"fr_en/fr_rel_triples_preprocessed\", \"fr_en/fr_att_triples_preprocessed\", \"fr_en/fr_combined_triples_folder2\")\n",
    "# ]\n",
    "\n",
    "# # Run batch processing\n",
    "# process_multiple_datasets(datasets_to_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate node descriptions (~ 2 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged graph created with 855133 triples.\n",
      "File saved: 'en_combined/part1.txt'.\n",
      "File saved: 'en_combined/part2.txt'.\n",
      "File saved: 'en_combined/part3.txt'.\n",
      "File saved: 'en_combined/part4.txt'.\n",
      "File saved: 'en_combined/part5.txt'.\n",
      "File saved: 'en_combined/part6.txt'.\n",
      "File saved: 'en_combined/part7.txt'.\n",
      "File saved: 'en_combined/part8.txt'.\n",
      "File saved: 'en_combined/part9.txt'.\n",
      "File saved: 'en_combined/part10.txt'.\n",
      "Merged graph created with 720856 triples.\n",
      "File saved: 'fr_combined/part1.txt'.\n",
      "File saved: 'fr_combined/part2.txt'.\n",
      "File saved: 'fr_combined/part3.txt'.\n",
      "File saved: 'fr_combined/part4.txt'.\n",
      "File saved: 'fr_combined/part5.txt'.\n",
      "File saved: 'fr_combined/part6.txt'.\n",
      "File saved: 'fr_combined/part7.txt'.\n",
      "File saved: 'fr_combined/part8.txt'.\n",
      "File saved: 'fr_combined/part9.txt'.\n",
      "File saved: 'fr_combined/part10.txt'.\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph\n",
    "import os\n",
    "\n",
    "def create_merged_graph(relation_file_path, attribute_file_path):\n",
    "    \"\"\"Creates and returns an RDF graph by merging relation and attribute files.\"\"\"\n",
    "    graph = Graph()\n",
    "    graph.parse(relation_file_path, format=\"nt\")\n",
    "    graph.parse(attribute_file_path, format=\"nt\")\n",
    "\n",
    "    print(f\"Merged graph created with {len(graph)} triples.\")\n",
    "    return graph\n",
    "\n",
    "def format_triples_for_embedding(graph, entity_uri, language_prefix):\n",
    "    \"\"\"\n",
    "    Formats RDF triples where the given entity is a subject or object into a readable format for embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    - graph: RDFLib Graph object\n",
    "    - entity_uri: URI of the entity to query for\n",
    "    - language_prefix: Prefix to indicate language (e.g., \"FR-\" or \"EN-\")\n",
    "\n",
    "    Returns:\n",
    "    - formatted_text: A single string containing all triples where the entity is subject or object, ready for embedding generation.\n",
    "    \"\"\"\n",
    "\n",
    "    def safe_split(uri):\n",
    "        \"\"\"Returns the language-prefixed label of a URI.\"\"\"\n",
    "        if uri is None:\n",
    "            return f\"{language_prefix}{entity_uri.split('/')[-1]}\"\n",
    "        return f\"{language_prefix}{uri.split('/')[-1]}\"\n",
    "\n",
    "    # Prepare SPARQL queries for both subject and object positions\n",
    "    query_subject = f\"\"\"\n",
    "    SELECT ?s ?p ?o\n",
    "    WHERE {{\n",
    "        <{entity_uri}> ?p ?o .\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    query_object = f\"\"\"\n",
    "    SELECT ?s ?p ?o\n",
    "    WHERE {{\n",
    "        ?s ?p <{entity_uri}> .\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute the queries\n",
    "    subject_results = graph.query(query_subject)\n",
    "    object_results = graph.query(query_object)\n",
    "\n",
    "    # Prepare the formatted text for embeddings\n",
    "    formatted_text = []\n",
    "\n",
    "    # Format triples where the entity is the subject\n",
    "    formatted_text.append(f\"# Triples where '{safe_split(entity_uri)}' is the subject:\\n\")\n",
    "    for s, p, o in subject_results:\n",
    "        formatted_text.append(f\"{safe_split(s)} {safe_split(p)} {safe_split(o)}.\")\n",
    "\n",
    "    # Format triples where the entity is the object\n",
    "    formatted_text.append(f\"\\n# Triples where '{safe_split(entity_uri)}' is the object:\\n\")\n",
    "    for s, p, o in object_results:\n",
    "        formatted_text.append(f\"{safe_split(s)} {safe_split(p)} {safe_split(o)}.\")\n",
    "\n",
    "    # Combine all triples into a single formatted string\n",
    "    formatted_text = \"\\n\".join(formatted_text)\n",
    "    return formatted_text\n",
    "\n",
    "def describe_node_for_embedding_per_subject(graph, output_file_prefix, language_prefix):\n",
    "    \"\"\"\n",
    "    Extracts all attributes and relations for every node and saves them in ten separate text files\n",
    "    (splitting the entities into ten equal parts for easier visualization).\n",
    "    \"\"\"\n",
    "    subjects = list(set(graph.subjects()))\n",
    "    total_subjects = len(subjects)\n",
    "    chunk_size = total_subjects // 10\n",
    "\n",
    "    os.makedirs(output_file_prefix, exist_ok=True)\n",
    "\n",
    "    # Split into ten files for easier management\n",
    "    for i in range(10):\n",
    "        start_index = i * chunk_size\n",
    "        end_index = (start_index + chunk_size) if (i < 9) else total_subjects\n",
    "        output_file = os.path.join(output_file_prefix, f\"part{i+1}.txt\")\n",
    "        \n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "            for subject in subjects[start_index:end_index]:\n",
    "                # Use the function to generate formatted triples for the subject\n",
    "                formatted_text = format_triples_for_embedding(graph, subject, language_prefix)\n",
    "                # Write the formatted data to the file with a separator for each entity\n",
    "                outfile.write(f\"\\n{'='*80}\\nEntity: {language_prefix}{subject.split('/')[-1]}\\n{'='*80}\\n\")\n",
    "                outfile.write(formatted_text + \"\\n\")\n",
    "        print(f\"File saved: '{output_file}'.\")\n",
    "\n",
    "def process_multiple_datasets(datasets):\n",
    "    \"\"\"Processes multiple RDF datasets and saves results for each in the specified folder.\"\"\"\n",
    "    for relation_file, attribute_file, output_folder, language_prefix in datasets:\n",
    "        graph = create_merged_graph(relation_file, attribute_file)\n",
    "        describe_node_for_embedding_per_subject(graph, output_folder, language_prefix)\n",
    "\n",
    "# List of datasets including both relation and attribute triples with the specified output folder and language prefix\n",
    "datasets_to_process = [\n",
    "    (\"fr_en/en_rel_triples_preprocessed\", \"fr_en/en_att_triples_preprocessed\", \"fr_en/en_combined\", \"EN-\"),\n",
    "    (\"fr_en/fr_rel_triples_preprocessed\", \"fr_en/fr_att_triples_preprocessed\", \"fr_en/fr_combined\", \"FR-\")\n",
    "]\n",
    "\n",
    "# Run batch processing\n",
    "process_multiple_datasets(datasets_to_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Pinecone index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinecone Index provisioned\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from pinecone import Pinecone, ServerlessSpec \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set your API keys for Pinecone\n",
    "pc = Pinecone(\n",
    "    api_key=os.environ['PINECONE_API_KEY']\n",
    ")\n",
    "\n",
    "# Create Index if not already created\n",
    "pinecone_index_name = \"dl-proj-4\"\n",
    "if pinecone_index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=pinecone_index_name, \n",
    "        dimension=1536, # '1536' is the dimension for text-embedding-3-small\n",
    "        metric='cosine',\n",
    "        spec=ServerlessSpec(\n",
    "            cloud='aws',\n",
    "            region='us-east-1'\n",
    "        )\n",
    "    )\n",
    "     \n",
    "    while not pc.describe_index(pinecone_index_name).index.status['ready']:\n",
    "        time.sleep(1)\n",
    "    \n",
    "    print(\"Pinecone Index provisioned\")\n",
    "else:\n",
    "    print(\"Pinecone Index Already Provisioned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English dataset conversion to embeddings\n",
    "Create and insert embeddings for the english dataset (~ 13 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings created, and inserted in Pinecone Vector Database successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set your API keys for OpenAI\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "# Initialize OpenAI Embeddings using LangChain\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")  # Specify which embedding model\n",
    "\n",
    "# Load all text files from a directory\n",
    "directory_path = \"fr_en/en_combined\"  # directory path with all the national weather service documents\n",
    "loader = DirectoryLoader(directory_path, glob=\"*.txt\", loader_cls=TextLoader)  # Load only .txt files\n",
    "documents = loader.load()\n",
    "\n",
    "# Use a TextSplitter to split the documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "split_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "# Connect to the Pinecone index using LangChain's Pinecone wrapper\n",
    "# Add all the split documents into the Pinecone vector database\n",
    "pinecone_index_name = \"dl-proj-4\"\n",
    "vectorstore = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings)\n",
    "vectorstore.add_documents(documents=split_documents )\n",
    "\n",
    "print(\"Embeddings created, and inserted in Pinecone Vector Database successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## French dataset conversion to embeddings\n",
    "Create and insert embeddings for the french dataset (~ 12 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings created, and inserted in Pinecone Vector Database successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set your API keys for OpenAI\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "# Initialize OpenAI Embeddings using LangChain\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")  # Specify which embedding model\n",
    "\n",
    "# Load all text files from a directory\n",
    "directory_path = \"fr_en/fr_combined\" \n",
    "loader = DirectoryLoader(directory_path, glob=\"*.txt\", loader_cls=TextLoader)  # Load only .txt files\n",
    "documents = loader.load()\n",
    "\n",
    "# Use a TextSplitter to split the documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=50)\n",
    "split_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "# Connect to the Pinecone index using LangChain's Pinecone wrapper\n",
    "# Add all the split documents into the Pinecone vector database\n",
    "pinecone_index_name = \"dl-proj-4\"\n",
    "vectorstore = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings)\n",
    "vectorstore.add_documents(documents=split_documents )\n",
    "\n",
    "print(\"Embeddings created, and inserted in Pinecone Vector Database successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving results from the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Marcus Tullius Cicero\n"
     ]
    }
   ],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os\n",
    "import openai\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set your API keys for OpenAI and Pinecone\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "# Initialize OpenAI Embeddings using LangChain\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")  # Specify which embedding model\n",
    "\n",
    "# Connect to the Pinecone index using LangChain's Pinecone wrapper\n",
    "pinecone_index_name = \"dl-proj-4\"\n",
    "vector_store = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings)\n",
    "\n",
    "# Define the retrieval mechanism\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})  # Retrieve top-3 relevant documents\n",
    "\n",
    "# Initialize GPT-4 with OpenAI\n",
    "llm = ChatOpenAI( model=\"gpt-4o-mini\", openai_api_key=openai.api_key, temperature=0.7 )\n",
    "\n",
    "# Define Prompt Template\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    Use the following context to answer the question as accurately as possible:\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "    Answer:\"\"\",\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Create LLM Chain\n",
    "llm_chain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "# Retrieve documents\n",
    "query = \"Which entity is most similar to Cicéron? Provide only entity name\"\n",
    "docs = retriever.invoke(query)\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    \n",
    "# Run LLM chain with the retrieved context\n",
    "answer = llm_chain.invoke({\"context\": context, \"question\": query})\n",
    "\n",
    "# Output the Answer and Sources\n",
    "print(\"Answer:\", answer)\n",
    "print(\"Context: \", context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aligning entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ? should there be any validation done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing evaluation metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
