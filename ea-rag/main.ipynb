{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Conda Virtual Environment\n",
    "To create a virtual environment using Conda, follow these steps:\n",
    "\n",
    "1. Open your terminal or command prompt.\n",
    "2. Run the following command to create a virtual environment named `ea-rag`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "%conda create --name ea-rag python=3.11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activate the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "conda activate ea_rag\n",
    "conda env list          # check that ea_rag* is activated \n",
    "python --version        # should be 3.11.x\n",
    "# from top right corner please select the corect environment and restart the kernel "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aiohappyeyeballs==2.4.4 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (2.4.4)\n",
      "Requirement already satisfied: aiohttp==3.9.5 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (3.9.5)\n",
      "Requirement already satisfied: aiosignal==1.3.2 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (1.3.2)\n",
      "Requirement already satisfied: annotated-types==0.7.0 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (0.7.0)\n",
      "Requirement already satisfied: anyio==4.7.0 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (4.7.0)\n",
      "Requirement already satisfied: appnope==0.1.4 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (0.1.4)\n",
      "Requirement already satisfied: asttokens==3.0.0 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (3.0.0)\n",
      "Requirement already satisfied: attrs==24.3.0 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 8)) (24.3.0)\n",
      "Requirement already satisfied: certifi==2024.12.14 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 9)) (2024.12.14)\n",
      "Requirement already satisfied: charset-normalizer==3.4.1 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 10)) (3.4.1)\n",
      "Requirement already satisfied: comm==0.2.2 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 11)) (0.2.2)\n",
      "Requirement already satisfied: dataclasses-json==0.6.7 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 12)) (0.6.7)\n",
      "Requirement already satisfied: debugpy==1.8.11 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 13)) (1.8.11)\n",
      "Requirement already satisfied: decorator==5.1.1 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 14)) (5.1.1)\n",
      "Requirement already satisfied: distro==1.9.0 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 15)) (1.9.0)\n",
      "Requirement already satisfied: executing==2.1.0 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 16)) (2.1.0)\n",
      "Requirement already satisfied: faiss-cpu==1.9.0.post1 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 17)) (1.9.0.post1)\n",
      "Requirement already satisfied: frozenlist==1.5.0 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 18)) (1.5.0)\n",
      "Requirement already satisfied: h11==0.14.0 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 19)) (0.14.0)\n",
      "Requirement already satisfied: httpcore==1.0.7 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 20)) (1.0.7)\n",
      "Requirement already satisfied: httpx==0.28.1 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 21)) (0.28.1)\n",
      "Requirement already satisfied: httpx-sse==0.4.0 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 22)) (0.4.0)\n",
      "Requirement already satisfied: idna==3.10 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 23)) (3.10)\n",
      "Requirement already satisfied: ipykernel==6.29.5 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 24)) (6.29.5)\n",
      "Requirement already satisfied: ipython==8.31.0 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 25)) (8.31.0)\n",
      "Requirement already satisfied: jedi==0.19.2 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 26)) (0.19.2)\n",
      "Requirement already satisfied: jiter==0.8.2 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 27)) (0.8.2)\n",
      "Requirement already satisfied: jsonpatch==1.33 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 28)) (1.33)\n",
      "Requirement already satisfied: jsonpointer==3.0.0 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 29)) (3.0.0)\n",
      "Requirement already satisfied: jupyter_client==8.6.3 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 30)) (8.6.3)\n",
      "Requirement already satisfied: jupyter_core==5.7.2 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 31)) (5.7.2)\n",
      "Requirement already satisfied: langchain==0.3.13 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 32)) (0.3.13)\n",
      "Requirement already satisfied: langchain-community==0.3.13 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 33)) (0.3.13)\n",
      "Requirement already satisfied: langchain-core==0.3.29 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 34)) (0.3.29)\n",
      "Requirement already satisfied: langchain-openai==0.2.14 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 35)) (0.2.14)\n",
      "Requirement already satisfied: langchain-pinecone==0.2.0 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 36)) (0.2.0)\n",
      "Requirement already satisfied: langchain-text-splitters==0.3.4 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 37)) (0.3.4)\n",
      "Requirement already satisfied: langsmith==0.2.8 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 38)) (0.2.8)\n",
      "Requirement already satisfied: marshmallow==3.23.2 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 39)) (3.23.2)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.7 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 40)) (0.1.7)\n",
      "Requirement already satisfied: multidict==6.1.0 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 41)) (6.1.0)\n",
      "Requirement already satisfied: mypy-extensions==1.0.0 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 42)) (1.0.0)\n",
      "Requirement already satisfied: nest-asyncio==1.6.0 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 43)) (1.6.0)\n",
      "Requirement already satisfied: numpy==1.26.4 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 44)) (1.26.4)\n",
      "Requirement already satisfied: openai==1.58.1 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 45)) (1.58.1)\n",
      "Requirement already satisfied: orjson==3.10.13 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 46)) (3.10.13)\n",
      "Requirement already satisfied: packaging==24.2 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 47)) (24.2)\n",
      "Requirement already satisfied: parso==0.8.4 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 48)) (0.8.4)\n",
      "Requirement already satisfied: pexpect==4.9.0 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 49)) (4.9.0)\n",
      "Requirement already satisfied: pinecone-client==5.0.1 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 50)) (5.0.1)\n",
      "Requirement already satisfied: pinecone-plugin-inference==1.1.0 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 51)) (1.1.0)\n",
      "Requirement already satisfied: pinecone-plugin-interface==0.0.7 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 52)) (0.0.7)\n",
      "Requirement already satisfied: platformdirs==4.3.6 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 53)) (4.3.6)\n",
      "Requirement already satisfied: prompt_toolkit==3.0.48 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 54)) (3.0.48)\n",
      "Requirement already satisfied: propcache==0.2.1 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 55)) (0.2.1)\n",
      "Requirement already satisfied: psutil==6.1.1 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 56)) (6.1.1)\n",
      "Requirement already satisfied: ptyprocess==0.7.0 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 57)) (0.7.0)\n",
      "Requirement already satisfied: pure_eval==0.2.3 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 58)) (0.2.3)\n",
      "Requirement already satisfied: pydantic==2.10.4 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 59)) (2.10.4)\n",
      "Requirement already satisfied: pydantic-settings==2.7.1 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 60)) (2.7.1)\n",
      "Requirement already satisfied: pydantic_core==2.27.2 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 61)) (2.27.2)\n",
      "Requirement already satisfied: Pygments==2.18.0 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 62)) (2.18.0)\n",
      "Requirement already satisfied: pyparsing==3.2.1 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 63)) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil==2.9.0.post0 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 64)) (2.9.0.post0)\n",
      "Requirement already satisfied: python-dotenv==1.0.1 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 65)) (1.0.1)\n",
      "Requirement already satisfied: PyYAML==6.0.2 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 66)) (6.0.2)\n",
      "Requirement already satisfied: pyzmq==26.2.0 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 67)) (26.2.0)\n",
      "Requirement already satisfied: rdflib==7.1.1 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 68)) (7.1.1)\n",
      "Requirement already satisfied: regex==2024.11.6 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 69)) (2024.11.6)\n",
      "Requirement already satisfied: requests==2.32.3 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 70)) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt==1.0.0 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 71)) (1.0.0)\n",
      "Requirement already satisfied: six==1.17.0 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 72)) (1.17.0)\n",
      "Requirement already satisfied: sniffio==1.3.1 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 73)) (1.3.1)\n",
      "Requirement already satisfied: SQLAlchemy==2.0.36 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 74)) (2.0.36)\n",
      "Requirement already satisfied: stack-data==0.6.3 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 75)) (0.6.3)\n",
      "Requirement already satisfied: tenacity==9.0.0 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 76)) (9.0.0)\n",
      "Requirement already satisfied: tiktoken==0.8.0 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 77)) (0.8.0)\n",
      "Requirement already satisfied: tornado==6.4.2 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 78)) (6.4.2)\n",
      "Requirement already satisfied: tqdm==4.67.1 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 79)) (4.67.1)\n",
      "Requirement already satisfied: traitlets==5.14.3 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 80)) (5.14.3)\n",
      "Requirement already satisfied: typing-inspect==0.9.0 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 81)) (0.9.0)\n",
      "Requirement already satisfied: typing_extensions==4.12.2 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 82)) (4.12.2)\n",
      "Requirement already satisfied: urllib3==2.3.0 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 83)) (2.3.0)\n",
      "Requirement already satisfied: wcwidth==0.2.13 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 84)) (0.2.13)\n",
      "Requirement already satisfied: yarl==1.18.3 in /Users/davide/anaconda3/envs/ea-rag/lib/python3.11/site-packages (from -r requirements.txt (line 85)) (1.18.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `.env` file with the following details:\n",
    "- `OPENAI_API_KEY`\n",
    "- `PINECONE_API_KEY`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the dataset\n",
    "The DBP15K dataset can be downloaded [here](https://huggingface.co/datasets/HackCz/DBP15K_raw/blob/main/DBP_raw.zip). Change the name of the foldder to 'fr_en'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relation triples\n",
    "Convert tabular data in `en_rel_triples` and `fr_rel_triples` to N-Triples format. Example:\n",
    "- input: `http://dbpedia.org/resource/Virton\thttp://dbpedia.org/property/nw\thttp://dbpedia.org/resource/Tintigny`\n",
    "- output: `<http://dbpedia.org/resource/Virton>\t<http://dbpedia.org/property/nw>\t<http://dbpedia.org/resource/Tintigny> .`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-Triples conversion complete for 'fr_en/en_rel_triples'. Output saved to 'fr_en/en_rel_triples_preprocessed'.\n",
      "N-Triples conversion complete for 'fr_en/fr_rel_triples'. Output saved to 'fr_en/fr_rel_triples_preprocessed'.\n"
     ]
    }
   ],
   "source": [
    "def convert_to_ntriples(input_file_path, output_file_path):\n",
    "    \"\"\"\n",
    "    Converts a TSV RDF dataset to N-Triples format.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_file_path (str): Path to the input TSV file.\n",
    "    - output_file_path (str): Path to save the processed N-Triples file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(input_file_path, \"r\", encoding=\"utf-8\") as infile, open(output_file_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "            for line in infile:\n",
    "                # Split each line using tab as the delimiter\n",
    "                parts = line.strip().split(\"\\t\")\n",
    "                \n",
    "                # Ensure the line has exactly 3 parts\n",
    "                if len(parts) == 3:\n",
    "                    subject, predicate, obj = parts\n",
    "                    # Format the line according to the N-Triples format\n",
    "                    n_triple_line = f\"<{subject}> <{predicate}> <{obj}> .\\n\"\n",
    "                    outfile.write(n_triple_line)\n",
    "                else:\n",
    "                    print(f\"Skipping invalid line: {line.strip()}\")\n",
    "\n",
    "        print(f\"N-Triples conversion complete for '{input_file_path}'. Output saved to '{output_file_path}'.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{input_file_path}' was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# List of datasets to process\n",
    "datasets = [\n",
    "    (\"fr_en/en_rel_triples\", \"fr_en/en_rel_triples_preprocessed\"),\n",
    "    (\"fr_en/fr_rel_triples\", \"fr_en/fr_rel_triples_preprocessed\")\n",
    "]\n",
    "\n",
    "# Loop through the datasets and process them\n",
    "for input_path, output_path in datasets:\n",
    "    convert_to_ntriples(input_path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attribute triples\n",
    "Converting negative dates to a format that can be processed using Python. Example:\n",
    "- input: `-0043-12-07`\n",
    "- output: `0043-12-07 BCE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed RDF data saved to: fr_en/en_att_triples_preprocessed\n",
      "Preprocessed RDF data saved to: fr_en/fr_att_triples_preprocessed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def preprocess_dates_in_file(input_file_path, output_file_path):\n",
    "    \"\"\"\n",
    "    Preprocesses the RDF file to convert negative xsd:date values to a BCE format as plain strings.\n",
    "\n",
    "    Parameters:\n",
    "    - input_file_path (str): Path to the original RDF file.\n",
    "    - output_file_path (str): Path to save the preprocessed RDF file.\n",
    "    \"\"\"\n",
    "    with open(input_file_path, \"r\", encoding=\"utf-8\") as infile, open(output_file_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        for line in infile:\n",
    "            # Detecting negative dates and converting them to string format\n",
    "            if '^^<http://www.w3.org/2001/XMLSchema#date>' in line:\n",
    "                start_index = line.find('\"') + 1\n",
    "                end_index = line.find('\"', start_index)\n",
    "                \n",
    "                if start_index != -1 and end_index != -1:\n",
    "                    date_string = line[start_index:end_index]\n",
    "                    if date_string.startswith(\"-\"):\n",
    "                        # Convert negative date to BCE string and remove type declaration\n",
    "                        sanitized_date = f'\"{date_string[1:]} BCE\"'\n",
    "                        # Remove the xsd:date type and keep it as a plain string\n",
    "                        line = line[:start_index-1] + sanitized_date + \" .\\n\"  # Added newline character\n",
    "            # Ensure each line ends with a newline character even if unmodified\n",
    "            if not line.endswith(\"\\n\"):\n",
    "                line += \"\\n\"\n",
    "            outfile.write(line)\n",
    "    \n",
    "    print(f\"Preprocessed RDF data saved to: {output_file_path}\")\n",
    "\n",
    "# Preprocess both datasets\n",
    "datasets_to_preprocess = [\n",
    "    (\"fr_en/en_att_triples\", \"fr_en/en_att_triples_preprocessed\"),\n",
    "    (\"fr_en/fr_att_triples\", \"fr_en/fr_att_triples_preprocessed\")\n",
    "]\n",
    "\n",
    "for input_file, output_file in datasets_to_preprocess:\n",
    "    preprocess_dates_in_file(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph creation & description generation\n",
    "Processes the datasets with the following steps:\n",
    "1) Creates a graph comprised of relation and attribute triples;\n",
    "2) Extracts all pieces of information about a node and stores it in a `.txt` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 107\u001b[0m\n\u001b[1;32m    101\u001b[0m datasets_to_process \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    102\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfr_en/en_rel_triples_preprocessed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfr_en/en_att_triples_preprocessed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfr_en/en_combined\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEN-\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    103\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfr_en/fr_rel_triples_preprocessed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfr_en/fr_att_triples_preprocessed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfr_en/fr_combined\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFR-\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    104\u001b[0m ]\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Run batch processing\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m \u001b[43mprocess_multiple_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatasets_to_process\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[89], line 97\u001b[0m, in \u001b[0;36mprocess_multiple_datasets\u001b[0;34m(datasets)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Processes multiple RDF datasets and saves results for each in the specified folder.\"\"\"\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m relation_file, attribute_file, output_folder, language_prefix \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[0;32m---> 97\u001b[0m     graph \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_merged_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrelation_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattribute_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     describe_node_for_embedding_per_subject(graph, output_folder, language_prefix)\n",
      "Cell \u001b[0;32mIn[89], line 8\u001b[0m, in \u001b[0;36mcreate_merged_graph\u001b[0;34m(relation_file_path, attribute_file_path)\u001b[0m\n\u001b[1;32m      6\u001b[0m graph \u001b[38;5;241m=\u001b[39m Graph()\n\u001b[1;32m      7\u001b[0m graph\u001b[38;5;241m.\u001b[39mparse(relation_file_path, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattribute_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMerged graph created with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(graph)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m triples.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph\n",
      "File \u001b[0;32m~/anaconda3/envs/ea-rag/lib/python3.11/site-packages/rdflib/graph.py:1502\u001b[0m, in \u001b[0;36mGraph.parse\u001b[0;34m(self, source, publicID, format, location, file, data, **args)\u001b[0m\n\u001b[1;32m   1499\u001b[0m     parser \u001b[38;5;241m=\u001b[39m plugin\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mformat\u001b[39m, Parser)()\n\u001b[1;32m   1500\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1501\u001b[0m     \u001b[38;5;66;03m# TODO FIXME: Parser.parse should have **kwargs argument.\u001b[39;00m\n\u001b[0;32m-> 1502\u001b[0m     \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mSyntaxError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m se:\n\u001b[1;32m   1504\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m could_not_guess_format:\n",
      "File \u001b[0;32m~/anaconda3/envs/ea-rag/lib/python3.11/site-packages/rdflib/plugins/parsers/ntriples.py:384\u001b[0m, in \u001b[0;36mNTParser.parse\u001b[0;34m(cls, source, sink, **kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m         f \u001b[38;5;241m=\u001b[39m codecs\u001b[38;5;241m.\u001b[39mgetreader(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)(b)\n\u001b[1;32m    383\u001b[0m parser \u001b[38;5;241m=\u001b[39m W3CNTriplesParser(NTGraphSink(sink))\n\u001b[0;32m--> 384\u001b[0m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m f\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/ea-rag/lib/python3.11/site-packages/rdflib/plugins/parsers/ntriples.py:198\u001b[0m, in \u001b[0;36mW3CNTriplesParser.parse\u001b[0;34m(self, f, bnode_context, skolemize)\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparseline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbnode_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnode_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParseError:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ParseError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid line: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mline))\n",
      "File \u001b[0;32m~/anaconda3/envs/ea-rag/lib/python3.11/site-packages/rdflib/plugins/parsers/ntriples.py:256\u001b[0m, in \u001b[0;36mW3CNTriplesParser.parseline\u001b[0;34m(self, bnode_context)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mline:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ParseError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrailing garbage: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mline))\n\u001b[0;32m--> 256\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msink\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtriple\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubject\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ea-rag/lib/python3.11/site-packages/rdflib/plugins/parsers/ntriples.py:351\u001b[0m, in \u001b[0;36mNTGraphSink.triple\u001b[0;34m(self, s, p, o)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtriple\u001b[39m(\u001b[38;5;28mself\u001b[39m, s: _SubjectType, p: _PredicateType, o: _ObjectType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 351\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ea-rag/lib/python3.11/site-packages/rdflib/graph.py:542\u001b[0m, in \u001b[0;36mGraph.add\u001b[0;34m(self, triple)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(p, Node), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicate \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m must be an rdflib term\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (p,)\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, Node), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObject \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m must be an rdflib term\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (o,)\n\u001b[0;32m--> 542\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquoted\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/ea-rag/lib/python3.11/site-packages/rdflib/plugins/stores/memory.py:370\u001b[0m, in \u001b[0;36mMemory.add\u001b[0;34m(self, triple, context, quoted)\u001b[0m\n\u001b[1;32m    368\u001b[0m osp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__osp\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 370\u001b[0m     sp \u001b[38;5;241m=\u001b[39m \u001b[43mosp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mobject_\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m    372\u001b[0m     sp \u001b[38;5;241m=\u001b[39m osp[object_] \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/anaconda3/envs/ea-rag/lib/python3.11/site-packages/rdflib/term.py:1277\u001b[0m, in \u001b[0;36mLiteral.__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1248\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1249\u001b[0m \u001b[38;5;124;03m>>> from rdflib.namespace import XSD\u001b[39;00m\n\u001b[1;32m   1250\u001b[0m \u001b[38;5;124;03m>>> a = {Literal('1', datatype=XSD.integer):'one'}\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1274\u001b[0m \n\u001b[1;32m   1275\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1276\u001b[0m \u001b[38;5;66;03m# don't use super()... for efficiency reasons, see Identifier.__hash__\u001b[39;00m\n\u001b[0;32m-> 1277\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mstr\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__hash__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;66;03m# Directly accessing the member is faster than the property.\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_language:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from rdflib import Graph\n",
    "import os\n",
    "\n",
    "def create_merged_graph(relation_file_path, attribute_file_path):\n",
    "    \"\"\"Creates and returns an RDF graph by merging relation and attribute files.\"\"\"\n",
    "    graph = Graph()\n",
    "    graph.parse(relation_file_path, format=\"nt\")\n",
    "    graph.parse(attribute_file_path, format=\"nt\")\n",
    "\n",
    "    print(f\"Merged graph created with {len(graph)} triples.\")\n",
    "    return graph\n",
    "\n",
    "def format_triples_for_embedding(graph, entity_uri, language_prefix):\n",
    "    \"\"\"\n",
    "    Formats RDF triples where the given entity is a subject or object into a readable format for embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    - graph: RDFLib Graph object\n",
    "    - entity_uri: URI of the entity to query for\n",
    "    - language_prefix: Prefix to indicate language (e.g., \"FR-\" or \"EN-\")\n",
    "\n",
    "    Returns:\n",
    "    - formatted_text: A single string containing all triples where the entity is subject or object, ready for embedding generation.\n",
    "    \"\"\"\n",
    "\n",
    "    def safe_split(uri):\n",
    "        \"\"\"Returns the language-prefixed label of a URI.\"\"\"\n",
    "        if uri is None:\n",
    "            return f\"{language_prefix}{entity_uri.split('/')[-1]}\"\n",
    "        return f\"{language_prefix}{uri.split('/')[-1]}\"\n",
    "\n",
    "    # Prepare SPARQL queries for both subject and object positions\n",
    "    query_subject = f\"\"\"\n",
    "    SELECT ?s ?p ?o\n",
    "    WHERE {{\n",
    "        <{entity_uri}> ?p ?o .\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    query_object = f\"\"\"\n",
    "    SELECT ?s ?p ?o\n",
    "    WHERE {{\n",
    "        ?s ?p <{entity_uri}> .\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute the queries\n",
    "    subject_results = graph.query(query_subject)\n",
    "    object_results = graph.query(query_object)\n",
    "\n",
    "    # Prepare the formatted text for embeddings\n",
    "    formatted_text = []\n",
    "\n",
    "    # Format triples where the entity is the subject\n",
    "    formatted_text.append(f\"# Triples where '{safe_split(entity_uri)}' is the subject:\\n\")\n",
    "    for s, p, o in subject_results:\n",
    "        formatted_text.append(f\"{safe_split(s)} {safe_split(p)} {safe_split(o)}.\")\n",
    "\n",
    "    # Format triples where the entity is the object\n",
    "    formatted_text.append(f\"\\n# Triples where '{safe_split(entity_uri)}' is the object:\\n\")\n",
    "    for s, p, o in object_results:\n",
    "        formatted_text.append(f\"{safe_split(s)} {safe_split(p)} {safe_split(o)}.\")\n",
    "\n",
    "    # Combine all triples into a single formatted string\n",
    "    formatted_text = \"\\n\".join(formatted_text)\n",
    "    return formatted_text\n",
    "\n",
    "def describe_node_for_embedding_per_subject(graph, output_file_prefix, language_prefix):\n",
    "    \"\"\"\n",
    "    Extracts all attributes and relations for every node and saves them in ten separate text files\n",
    "    (splitting the entities into ten equal parts for easier visualization).\n",
    "    \"\"\"\n",
    "    subjects = list(set(graph.subjects()))\n",
    "    total_subjects = len(subjects)\n",
    "    chunk_size = total_subjects // 10\n",
    "\n",
    "    os.makedirs(output_file_prefix, exist_ok=True)\n",
    "\n",
    "    # Split into ten files for easier management\n",
    "    for i in range(10):\n",
    "        start_index = i * chunk_size\n",
    "        end_index = (start_index + chunk_size) if (i < 9) else total_subjects\n",
    "        output_file = os.path.join(output_file_prefix, f\"part{i+1}.txt\")\n",
    "        \n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "            for subject in subjects[start_index:end_index]:\n",
    "                # Use the function to generate formatted triples for the subject\n",
    "                formatted_text = format_triples_for_embedding(graph, subject, language_prefix)\n",
    "                # Write the formatted data to the file with a separator for each entity\n",
    "                outfile.write(f\"\\n{'='*80}\\nEntity: {language_prefix}{subject.split('/')[-1]}\\n{'='*80}\\n\")\n",
    "                outfile.write(formatted_text + \"\\n\")\n",
    "        print(f\"File saved: '{output_file}'.\")\n",
    "\n",
    "def process_multiple_datasets(datasets):\n",
    "    \"\"\"Processes multiple RDF datasets and saves results for each in the specified folder.\"\"\"\n",
    "    for relation_file, attribute_file, output_folder, language_prefix in datasets:\n",
    "        graph = create_merged_graph(relation_file, attribute_file)\n",
    "        describe_node_for_embedding_per_subject(graph, output_folder, language_prefix)\n",
    "\n",
    "# List of datasets including both relation and attribute triples with the specified output folder and language prefix\n",
    "datasets_to_process = [\n",
    "    (\"fr_en/en_rel_triples_preprocessed\", \"fr_en/en_att_triples_preprocessed\", \"fr_en/en_combined\", \"EN-\"),\n",
    "    (\"fr_en/fr_rel_triples_preprocessed\", \"fr_en/fr_att_triples_preprocessed\", \"fr_en/fr_combined\", \"FR-\")\n",
    "]\n",
    "\n",
    "# Run batch processing\n",
    "process_multiple_datasets(datasets_to_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Pinecone index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinecone Index provisioned\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from pinecone import Pinecone, ServerlessSpec \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set your API keys for Pinecone\n",
    "pc = Pinecone(\n",
    "    api_key=os.environ['PINECONE_API_KEY']\n",
    ")\n",
    "\n",
    "# Create Index if not already created\n",
    "pinecone_index_name = \"dl-proj-4\"\n",
    "if pinecone_index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=pinecone_index_name, \n",
    "        dimension=1536, # '1536' is the dimension for text-embedding-3-small\n",
    "        metric='cosine',\n",
    "        spec=ServerlessSpec(\n",
    "            cloud='aws',\n",
    "            region='us-east-1'\n",
    "        )\n",
    "    )\n",
    "     \n",
    "    while not pc.describe_index(pinecone_index_name).index.status['ready']:\n",
    "        time.sleep(1)\n",
    "    \n",
    "    print(\"✅ Pinecone Index provisioned\")\n",
    "else:\n",
    "    print(\"Pinecone Index Already Provisioned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English dataset conversion to embeddings\n",
    "Create and insert embeddings for the english dataset (~ 19 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings created, and inserted in Pinecone Vector Database successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set your API keys for OpenAI\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "# Initialize OpenAI Embeddings using LangChain\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")  # Specify which embedding model\n",
    "\n",
    "# Load all text files from a directory\n",
    "directory_path = \"fr_en/en_combined\"  # directory path with all the national weather service documents\n",
    "loader = DirectoryLoader(directory_path, glob=\"*.txt\", loader_cls=TextLoader)  # Load only .txt files\n",
    "documents = loader.load()\n",
    "\n",
    "# Use a TextSplitter to split the documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "split_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "# Connect to the Pinecone index using LangChain's Pinecone wrapper\n",
    "# Add all the split documents into the Pinecone vector database\n",
    "pinecone_index_name = \"dl-proj-4\"\n",
    "vectorstore = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings)\n",
    "vectorstore.add_documents(documents=split_documents )\n",
    "\n",
    "print(\"🇬🇧 Embeddings created and inserted in Pinecone Vector Database successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### French dataset conversion to embeddings\n",
    "Create and insert embeddings for the french dataset (~ 18 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings created, and inserted in Pinecone Vector Database successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set your API keys for OpenAI\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "# Initialize OpenAI Embeddings using LangChain\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")  # Specify which embedding model\n",
    "\n",
    "# Load all text files from a directory\n",
    "directory_path = \"fr_en/fr_combined\" \n",
    "loader = DirectoryLoader(directory_path, glob=\"*.txt\", loader_cls=TextLoader)  # Load only .txt files\n",
    "documents = loader.load()\n",
    "\n",
    "# Use a TextSplitter to split the documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "split_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "# Connect to the Pinecone index using LangChain's Pinecone wrapper\n",
    "# Add all the split documents into the Pinecone vector database\n",
    "pinecone_index_name = \"dl-proj-4\"\n",
    "vectorstore = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings)\n",
    "vectorstore.add_documents(documents=split_documents )\n",
    "\n",
    "print(\"🇫🇷 Embeddings created and inserted in Pinecone Vector Database successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating predictions\n",
    "Uses the `gpt-4o-mini` pre-trained Large Language Model through OpenAI's API. The model receives the context retrieved from the Pinecone index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset\n",
    "Splits the dataset into 10 smaller parts for easier processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(file_path, output_folder=\"fr_en/split_parts\", num_parts=10):\n",
    "    \"\"\"Split a large dataset into smaller parts for batch processing with multiple API keys.\n",
    "    \n",
    "    Args:\n",
    "    - file_path (str): Path to the dataset file.\n",
    "    - output_folder (str): Folder to save the split parts.\n",
    "    - num_parts (int): Number of parts to split the dataset into.\n",
    "    \"\"\"\n",
    "    import os\n",
    "\n",
    "    # Ensure the output directory exists and is empty\n",
    "    if os.path.exists(output_folder):\n",
    "        # Clear the folder before saving new parts\n",
    "        for file in os.listdir(output_folder):\n",
    "            os.remove(os.path.join(output_folder, file))\n",
    "    else:\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Read all lines from the dataset\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Calculate the number of lines per part\n",
    "    total_lines = len(lines)\n",
    "    lines_per_part = total_lines // num_parts\n",
    "    print(f\"Total lines: {total_lines}, Lines per part: {lines_per_part}\")\n",
    "\n",
    "    # Split the dataset into multiple parts\n",
    "    for i in range(num_parts):\n",
    "        start_idx = i * lines_per_part\n",
    "        # For the last file, include all remaining lines\n",
    "        end_idx = start_idx + lines_per_part if i < num_parts - 1 else total_lines\n",
    "        part_lines = lines[start_idx:end_idx]\n",
    "\n",
    "        # Save each part as a separate file in the specified folder\n",
    "        part_file_path = os.path.join(output_folder, f\"ent_ILLs_part_{i + 1}.txt\")\n",
    "        with open(part_file_path, \"w\", encoding=\"utf-8\") as part_file:\n",
    "            part_file.writelines(part_lines)\n",
    "        print(f\"✅ Part {i + 1} saved to '{part_file_path}' with {len(part_lines)} lines.\")\n",
    "\n",
    "# Usage Example\n",
    "split_dataset(\"fr_en/ent_ILLs_medium.txt\", num_parts=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying the model\n",
    "Uses 5 API keys to avoid hitting the rate limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Processing: fr_en/split_parts/ent_ILLs_part_1.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [05:21<00:00,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Results saved to fr_en/aligned_results/ent_ILLs_part_1_aligned.txt\n",
      "\n",
      "🚀 Processing: fr_en/split_parts/ent_ILLs_part_10.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [05:30<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Results saved to fr_en/aligned_results/ent_ILLs_part_10_aligned.txt\n",
      "\n",
      "🚀 Processing: fr_en/split_parts/ent_ILLs_part_2.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [05:12<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Results saved to fr_en/aligned_results/ent_ILLs_part_2_aligned.txt\n",
      "\n",
      "🚀 Processing: fr_en/split_parts/ent_ILLs_part_3.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [05:11<00:00,  4.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Results saved to fr_en/aligned_results/ent_ILLs_part_3_aligned.txt\n",
      "\n",
      "🚀 Processing: fr_en/split_parts/ent_ILLs_part_4.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [05:45<00:00,  4.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Results saved to fr_en/aligned_results/ent_ILLs_part_4_aligned.txt\n",
      "\n",
      "🚀 Processing: fr_en/split_parts/ent_ILLs_part_5.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [05:20<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Results saved to fr_en/aligned_results/ent_ILLs_part_5_aligned.txt\n",
      "\n",
      "🚀 Processing: fr_en/split_parts/ent_ILLs_part_6.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [05:30<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Results saved to fr_en/aligned_results/ent_ILLs_part_6_aligned.txt\n",
      "\n",
      "🚀 Processing: fr_en/split_parts/ent_ILLs_part_7.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [05:18<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Results saved to fr_en/aligned_results/ent_ILLs_part_7_aligned.txt\n",
      "\n",
      "🚀 Processing: fr_en/split_parts/ent_ILLs_part_8.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [05:21<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Results saved to fr_en/aligned_results/ent_ILLs_part_8_aligned.txt\n",
      "\n",
      "🚀 Processing: fr_en/split_parts/ent_ILLs_part_9.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [12:24<00:00,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Results saved to fr_en/aligned_results/ent_ILLs_part_9_aligned.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import tqdm\n",
    "import time\n",
    "import threading\n",
    "import glob\n",
    "\n",
    "# Load environment variables for multiple API keys\n",
    "load_dotenv()\n",
    "API_KEYS = [\n",
    "    os.environ.get('OPENAI_API_KEY_01'),\n",
    "    os.environ.get('OPENAI_API_KEY_02'),\n",
    "    os.environ.get('OPENAI_API_KEY_03'),\n",
    "    os.environ.get('OPENAI_API_KEY_04'),\n",
    "    os.environ.get('OPENAI_API_KEY_05')\n",
    "]\n",
    "\n",
    "# Initialize token buckets for rate and daily limits\n",
    "RATE_LIMIT = 500  # per minute per API key\n",
    "DAILY_LIMIT = 10000  # per day per API key\n",
    "api_key_index = 0\n",
    "rate_limit_buckets = [threading.Semaphore(RATE_LIMIT) for _ in API_KEYS]\n",
    "daily_token_buckets = [threading.Semaphore(DAILY_LIMIT) for _ in API_KEYS]\n",
    "lock = threading.Lock()  # For thread-safe key rotation\n",
    "\n",
    "def refill_tokens():\n",
    "    \"\"\"Refill the rate limit buckets every minute.\"\"\"\n",
    "    while True:\n",
    "        time.sleep(60)\n",
    "        for bucket in rate_limit_buckets:\n",
    "            for _ in range(RATE_LIMIT):\n",
    "                bucket.release()\n",
    "\n",
    "def reset_daily_tokens():\n",
    "    \"\"\"Reset the daily token buckets at midnight.\"\"\"\n",
    "    while True:\n",
    "        now = time.localtime()\n",
    "        seconds_until_midnight = (24 * 3600) - (now.tm_hour * 3600 + now.tm_min * 60 + now.tm_sec)\n",
    "        time.sleep(seconds_until_midnight)\n",
    "        for bucket in daily_token_buckets:\n",
    "            for _ in range(DAILY_LIMIT):\n",
    "                bucket.release()\n",
    "\n",
    "# Start the token refill threads\n",
    "threading.Thread(target=refill_tokens, daemon=True).start()\n",
    "threading.Thread(target=reset_daily_tokens, daemon=True).start()\n",
    "\n",
    "def get_next_api_key():\n",
    "    \"\"\"Rotate between API keys with thread safety.\"\"\"\n",
    "    global api_key_index\n",
    "    with lock:\n",
    "        api_key_index = (api_key_index + 1) % len(API_KEYS)\n",
    "    return API_KEYS[api_key_index], rate_limit_buckets[api_key_index], daily_token_buckets[api_key_index]\n",
    "\n",
    "def initialize_langchain(api_key):\n",
    "    \"\"\"Initialize LangChain components with a specific API key.\"\"\"\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", openai_api_key=api_key)\n",
    "    pinecone_index_name = \"dl-proj-4\"\n",
    "    vector_store = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings)\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", openai_api_key=api_key, temperature=0.4)\n",
    "\n",
    "    prompt_template = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        Use the following context to identify the most similar entity in the French dataset (prefixed with FR-) for the given entity in the English dataset (prefixed with EN-):\n",
    "        Context: {context}\n",
    "        Given Entity: {question}\n",
    "        Provide only the name of the most similar entity from the dataset, prefixed with 'FR-'. Use the entity name, the provided context and your knowledge to identify the best answer.\n",
    "        Answer:\"\"\",\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "    llm_chain = prompt_template | llm | StrOutputParser()\n",
    "    return retriever, llm_chain\n",
    "\n",
    "def process_file(file_path):\n",
    "    \"\"\"Read the input file and return the modified entities for both EN and FR.\"\"\"\n",
    "    en_entities = []\n",
    "    fr_entities = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) == 2:\n",
    "                _, uri = parts\n",
    "                modified_uri = uri.replace(\"http://dbpedia.org/resource/\", \"\")\n",
    "                if uri.startswith(\"http://dbpedia.org/resource/FR-\"):\n",
    "                    fr_entities.append(f\"FR-{modified_uri}\")\n",
    "                else:\n",
    "                    en_entities.append(f\"EN-{modified_uri}\")\n",
    "    return en_entities, fr_entities\n",
    "\n",
    "def process_single_entity(entity, retries=3, delay=5):\n",
    "    \"\"\"Process a single entity with retries on failure.\"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            # API Key Handling\n",
    "            api_key, rate_bucket, daily_bucket = get_next_api_key()\n",
    "            rate_bucket.acquire()\n",
    "            daily_bucket.acquire()\n",
    "            \n",
    "            # Initialize LangChain Components\n",
    "            retriever, llm_chain = initialize_langchain(api_key)\n",
    "            \n",
    "            # Query Preparation\n",
    "            query = f\"{entity}\"\n",
    "            docs = retriever.invoke(query)\n",
    "            context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "            # Query the API\n",
    "            answer = llm_chain.invoke({\"context\": context, \"question\": entity})\n",
    "            if not answer: \n",
    "                print(f\"Empty response for {entity}\")\n",
    "                continue  # Retry if the answer is empty\n",
    "\n",
    "            # Prepare Result URIs\n",
    "            english_uri = f\"http://dbpedia.org/resource/{entity.replace('EN-', '')}\"\n",
    "            french_uri = f\"http://fr.dbpedia.org/resource/{answer.replace('FR-', '').replace(' ', '_')}\"\n",
    "            return english_uri, french_uri\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing entity {entity} (attempt {attempt + 1}/{retries}): {e}\")\n",
    "            time.sleep(delay)  # Wait before retrying\n",
    "\n",
    "    # If all retries fail, return None and log it\n",
    "    print(f\"❌ Failed to process {entity} after {retries} attempts.\")\n",
    "    return None, None\n",
    "\n",
    "def save_failed_entities(failed_entities, output_file):\n",
    "    \"\"\"Save the entities that failed processing to a file for later reprocessing.\"\"\"\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        for entity in failed_entities:\n",
    "            file.write(f\"{entity}\\n\")\n",
    "    print(f\"\\n❌ Failed entities saved to {output_file}\")\n",
    "\n",
    "# In the parallel processing function, collect failed entities\n",
    "def query_llm_for_entity_pairing_parallel(en_entities, max_workers=10):\n",
    "    results = {}\n",
    "    failed_entities = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(process_single_entity, entity): entity for entity in en_entities}\n",
    "\n",
    "        for future in tqdm.tqdm(as_completed(futures), total=len(futures)):\n",
    "            try:\n",
    "                english_uri, french_uri = future.result()\n",
    "                if english_uri and french_uri:\n",
    "                    results[english_uri] = french_uri\n",
    "                else:\n",
    "                    failed_entities.append(futures[future])\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                failed_entities.append(futures[future])\n",
    "\n",
    "    # Save failed entities for retry later\n",
    "    if failed_entities:\n",
    "        save_failed_entities(failed_entities, \"fr_en/failed_entities.txt\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def save_results_to_txt(results, output_file):\n",
    "    \"\"\"Save the alignment results to a .txt file.\"\"\"\n",
    "    # Ensure the directory exists before saving\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        for en_uri, fr_uri in results.items():\n",
    "            file.write(f\"{fr_uri}\\t{en_uri}\\n\")\n",
    "    print(f\"\\n✅ Results saved to {output_file}\")\n",
    "\n",
    "# === Process All Split Files ===\n",
    "split_files = sorted(glob.glob(\"fr_en/split_parts/ent_ILLs_part*.txt\"))\n",
    "\n",
    "for split_file in split_files:\n",
    "    output_file = split_file.replace(\"split_parts\", \"aligned_results\").replace(\".txt\", \"_aligned.txt\")\n",
    "    \n",
    "    # Skip if already processed\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"✅ {split_file} already processed. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n🚀 Processing: {split_file}\")\n",
    "    \n",
    "    # Load the current part\n",
    "    en_entities, fr_entities = process_file(split_file)\n",
    "\n",
    "    # Perform parallel processing\n",
    "    results = query_llm_for_entity_pairing_parallel(en_entities, max_workers=10)\n",
    "\n",
    "    # Save results immediately after processing each file\n",
    "    save_results_to_txt(results, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity comparison\n",
    "Compares the predictions generated by the LLM with the ground truth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction merging\n",
    "Combines the predictions generated by the model into a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging ent_ILLs_part_1_aligned.txt...\n",
      "Merging ent_ILLs_part_2_aligned.txt...\n",
      "Merging ent_ILLs_part_3_aligned.txt...\n",
      "Merging ent_ILLs_part_4_aligned.txt...\n",
      "Merging ent_ILLs_part_5_aligned.txt...\n",
      "Merging ent_ILLs_part_6_aligned.txt...\n",
      "Merging ent_ILLs_part_7_aligned.txt...\n",
      "Merging ent_ILLs_part_8_aligned.txt...\n",
      "Merging ent_ILLs_part_9_aligned.txt...\n",
      "Merging ent_ILLs_part_10_aligned.txt...\n",
      "Merging complete! Results saved to fr_en/aligned_entities.txt.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the directories and output file path\n",
    "input_directory = \"fr_en/aligned_results/\"\n",
    "output_file = \"fr_en/aligned_entities.txt\"\n",
    "\n",
    "# Open the output file in write mode\n",
    "with open(output_file, \"w\") as outfile:\n",
    "    for i in range(1, 11):  # Loop from 1 to 10\n",
    "        filename = f\"ent_ILLs_part_{i}_aligned.txt\"\n",
    "        filepath = os.path.join(input_directory, filename)\n",
    "        \n",
    "        # Check if the file exists before merging\n",
    "        if os.path.exists(filepath):\n",
    "            print(f\"⏳ Merging {filename}\")\n",
    "            with open(filepath, \"r\") as infile:\n",
    "                outfile.write(infile.read())\n",
    "                outfile.write(\"\\n\")  # Add a newline between merged contents\n",
    "        else:\n",
    "            print(f\"File {filename} not found!\")\n",
    "\n",
    "print(f\"✅ Merging complete. Results saved to {output_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hits@1\n",
    "Computes the hits@1 score and saves non-aligned entities inside a `.txt` file. Both the normalized and original URIs are saved inside the `fr_en` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Hits@1 Score: 0.6465\n",
      "❌ Non-aligned normalized entities saved to: fr_en/non_aligned_entities_normalized.txt\n",
      "❌ Non-aligned original entities saved to: fr_en/non_aligned_entities_original.txt\n",
      "Total entities in ground truth: 15000\n",
      "Total entities in alignment results: 15000\n",
      "Matching keys count: 15000\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "# Standardize all abbreviations to the same form\n",
    "ABBREVIATION_MAP = {\n",
    "    \"f.c.\": \"football_club\",\n",
    "    \"fc\": \"football_club\",\n",
    "    \"football club\": \"football_club\",\n",
    "    \"univ.\": \"university\",\n",
    "    \"univ\": \"university\",\n",
    "    \"co.\": \"company\",\n",
    "    \"co\": \"company\",\n",
    "    \"corp.\": \"corporation\",\n",
    "    \"corp\": \"corporation\"\n",
    "}\n",
    "\n",
    "def normalize_uri(uri):\n",
    "    \"\"\"\n",
    "    Normalize a URI by:\n",
    "    - Removing parentheses (but keeping the content inside)\n",
    "    - Trimming whitespace\n",
    "    - Lowercasing\n",
    "    - Removing accents from characters\n",
    "    - Standardizing both '-' and '_' to '_'\n",
    "    - Replacing all abbreviations with consistent forms\n",
    "    - Sorting words **only after the base URL**\n",
    "    \"\"\"\n",
    "    original_uri = uri.strip()\n",
    "    uri = uri.strip().lower()\n",
    "    \n",
    "    # Remove parentheses but keep their content\n",
    "    uri = uri.replace(\"(\", \"\").replace(\")\", \"\")\n",
    "\n",
    "    # Convert accented characters to base form (removes accents)\n",
    "    uri = unicodedata.normalize(\"NFD\", uri)\n",
    "    uri = \"\".join(char for char in uri if unicodedata.category(char) != 'Mn')  # Remove accents\n",
    "\n",
    "    # Replace all abbreviations with consistent forms\n",
    "    for abbr, full_form in ABBREVIATION_MAP.items():\n",
    "        uri = uri.replace(abbr, full_form)\n",
    "\n",
    "    # Normalize separators: Convert both '-' and '_' into a single '_'\n",
    "    uri = re.sub(r\"[-_]+\", \"_\", uri)  # Merge consecutive separators into one\n",
    "\n",
    "    # Split base URL and resource part for separate handling\n",
    "    if \"/resource/\" in uri:\n",
    "        base_url, entity = uri.split(\"/resource/\", 1)\n",
    "        # Sort the words **only for the entity part**\n",
    "        words = entity.split(\"_\")\n",
    "        words_sorted = sorted(words)\n",
    "        entity_normalized = \"_\".join(words_sorted).strip(\"_\")\n",
    "        uri = f\"{base_url}/resource/{entity_normalized}\"\n",
    "    else:\n",
    "        # If no base URL detected, normalize the whole string\n",
    "        words = uri.split(\"_\")\n",
    "        words_sorted = sorted(words)\n",
    "        uri = \"_\".join(words_sorted).strip(\"_\")\n",
    "    \n",
    "    return uri, original_uri  # Return both normalized and original versions\n",
    "\n",
    "def read_alignment_file(file_path):\n",
    "    \"\"\"\n",
    "    Read and normalize the alignment results file.\n",
    "    \"\"\"\n",
    "    alignment_results = {}\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) == 2:\n",
    "                fr_uri, en_uri = parts\n",
    "                # Normalize both URIs and store original versions too\n",
    "                fr_uri_normalized, fr_uri_original = normalize_uri(fr_uri)\n",
    "                en_uri_normalized, en_uri_original = normalize_uri(en_uri)\n",
    "                alignment_results[en_uri_normalized] = (fr_uri_normalized, fr_uri_original, en_uri_original)\n",
    "            else:\n",
    "                print(f\"Skipping invalid line: {line.strip()}\")\n",
    "    return alignment_results\n",
    "\n",
    "def read_ground_truth_file(file_path):\n",
    "    \"\"\"\n",
    "    Read and normalize the ground truth file.\n",
    "    \"\"\"\n",
    "    ground_truth = {}\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) == 2:\n",
    "                fr_uri, en_uri = parts\n",
    "                # Normalize both URIs and store original versions too\n",
    "                fr_uri_normalized, fr_uri_original = normalize_uri(fr_uri)\n",
    "                en_uri_normalized, en_uri_original = normalize_uri(en_uri)\n",
    "                ground_truth[en_uri_normalized] = (fr_uri_normalized, fr_uri_original, en_uri_original)\n",
    "            else:\n",
    "                print(f\"Skipping invalid line: {line.strip()}\")\n",
    "    return ground_truth\n",
    "\n",
    "def compute_hits_at_1_and_save_misses(alignment_results, ground_truth):\n",
    "    \"\"\"\n",
    "    Compute Hits@1 and save non-aligned entities to separate files:\n",
    "    - One for normalized results\n",
    "    - One for original results\n",
    "    \"\"\"\n",
    "    hits = 0\n",
    "    compared_entities = 0\n",
    "    non_aligned_normalized = []\n",
    "    non_aligned_original = []\n",
    "\n",
    "    # Adding headers for both files\n",
    "    non_aligned_normalized.append(\"English URI (normalized)\\tGround Truth French URI (normalized)\\tPredicted French URI (normalized)\")\n",
    "    non_aligned_original.append(\"English URI (original)\\tGround Truth French URI (original)\\tPredicted French URI (original)\")\n",
    "\n",
    "    for en_entity, (actual_fr_entity_norm, actual_fr_entity_orig, en_entity_orig) in ground_truth.items():\n",
    "        if en_entity in alignment_results:  # Only compare if the entity exists in both sets\n",
    "            compared_entities += 1\n",
    "            predicted_fr_entity_norm, predicted_fr_entity_orig, predicted_en_entity_orig = alignment_results[en_entity]\n",
    "            if predicted_fr_entity_norm == actual_fr_entity_norm:\n",
    "                hits += 1\n",
    "            else:\n",
    "                non_aligned_normalized.append(f\"{en_entity}\\t{actual_fr_entity_norm}\\t{predicted_fr_entity_norm or 'Not Found'}\")\n",
    "                non_aligned_original.append(f\"{en_entity_orig}\\t{actual_fr_entity_orig}\\t{predicted_fr_entity_orig or 'Not Found'}\")\n",
    "        else:\n",
    "            non_aligned_normalized.append(f\"{en_entity}\\t{actual_fr_entity_norm}\\tNot Found\")\n",
    "            non_aligned_original.append(f\"{en_entity_orig}\\t{actual_fr_entity_orig}\\tNot Found\")\n",
    "\n",
    "    # Save normalized results\n",
    "    normalized_file = \"fr_en/non_aligned_entities_normalized.txt\"\n",
    "    with open(normalized_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(\"\\n\".join(non_aligned_normalized))\n",
    "    \n",
    "    # Save original results\n",
    "    original_file = \"fr_en/non_aligned_entities_original.txt\"\n",
    "    with open(original_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(\"\\n\".join(non_aligned_original))\n",
    "    \n",
    "    # Print Hits@1 Score\n",
    "    print(f\"\\n✅ Hits@1 Score: {hits / compared_entities:.4f}\")\n",
    "    print(f\"❌ Non-aligned normalized entities saved to: {normalized_file}\")\n",
    "    print(f\"❌ Non-aligned original entities saved to: {original_file}\")\n",
    "    return hits / compared_entities\n",
    "\n",
    "# === Running the entire process with strict normalization ===\n",
    "alignment_results_file = \"fr_en/aligned_entities.txt\"\n",
    "ground_truth_file = \"fr_en/ent_ILLs.txt\"\n",
    "\n",
    "alignment_results = read_alignment_file(alignment_results_file)\n",
    "ground_truth = read_ground_truth_file(ground_truth_file)\n",
    "hits_at_1_score = compute_hits_at_1_and_save_misses(alignment_results, ground_truth)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ea-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
