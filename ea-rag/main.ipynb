{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Conda Virtual Environment\n",
    "To create a virtual environment using Conda, follow these steps:\n",
    "\n",
    "1. Open your terminal or command prompt.\n",
    "2. Run the following command to create a virtual environment named `ea-rag`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "%conda create --name ea-rag python=3.11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activate the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "conda activate ea_rag\n",
    "conda env list          # check that ea_rag* is activated \n",
    "python --version        # should be 3.11.x\n",
    "# from top right corner please select the corect environment and restart the kernel "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `.env` file with the following details:\n",
    "- `OPENAI_API_KEY01`\n",
    "- `OPENAI_API_KEY02`\n",
    "- `OPENAI_API_KEY03`\n",
    "- `OPENAI_API_KEY04`\n",
    "- `OPENAI_API_KEY05`\n",
    "- `PINECONE_API_KEY`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the dataset\n",
    "The DBP15K dataset can be downloaded [here](https://huggingface.co/datasets/HackCz/DBP15K_raw/blob/main/DBP_raw.zip). Unarchive the 'DBP_raw.zip' file, open the extracted folder ('DBP15k_raw_all'), unarchive the 'DBP15k.zip' file, open the 'DBP15k_raw' folder and copy the 'fr_en' folder inside the folder you are currenly working in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relation triples\n",
    "Convert tabular data in `en_rel_triples` and `fr_rel_triples` to N-Triples format. Example:\n",
    "- input: `http://dbpedia.org/resource/Virton\thttp://dbpedia.org/property/nw\thttp://dbpedia.org/resource/Tintigny`\n",
    "- output: `<http://dbpedia.org/resource/Virton>\t<http://dbpedia.org/property/nw>\t<http://dbpedia.org/resource/Tintigny> .`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_ntriples(input_file_path, output_file_path):\n",
    "    \"\"\"\n",
    "    Converts a TSV RDF dataset to N-Triples format.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_file_path (str): Path to the input TSV file.\n",
    "    - output_file_path (str): Path to save the processed N-Triples file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(input_file_path, \"r\", encoding=\"utf-8\") as infile, open(output_file_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "            for line in infile:\n",
    "                # Split each line using tab as the delimiter\n",
    "                parts = line.strip().split(\"\\t\")\n",
    "                \n",
    "                # Ensure the line has exactly 3 parts\n",
    "                if len(parts) == 3:\n",
    "                    subject, predicate, obj = parts\n",
    "                    # Format the line according to the N-Triples format\n",
    "                    n_triple_line = f\"<{subject}> <{predicate}> <{obj}> .\\n\"\n",
    "                    outfile.write(n_triple_line)\n",
    "                else:\n",
    "                    print(f\"Skipping invalid line: {line.strip()}\")\n",
    "\n",
    "        print(f\"N-Triples conversion complete for '{input_file_path}'. Output saved to '{output_file_path}'.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{input_file_path}' was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# List of datasets to process\n",
    "datasets = [\n",
    "    (\"fr_en/en_rel_triples\", \"fr_en/en_rel_triples_preprocessed\"),\n",
    "    (\"fr_en/fr_rel_triples\", \"fr_en/fr_rel_triples_preprocessed\")\n",
    "]\n",
    "\n",
    "# Loop through the datasets and process them\n",
    "for input_path, output_path in datasets:\n",
    "    convert_to_ntriples(input_path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attribute triples\n",
    "Converting negative dates to a format that can be processed using Python. Example:\n",
    "- input: `-0043-12-07`\n",
    "- output: `0043-12-07 BCE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def preprocess_dates_in_file(input_file_path, output_file_path):\n",
    "    \"\"\"\n",
    "    Preprocesses the RDF file to convert negative xsd:date values to a BCE format as plain strings.\n",
    "\n",
    "    Parameters:\n",
    "    - input_file_path (str): Path to the original RDF file.\n",
    "    - output_file_path (str): Path to save the preprocessed RDF file.\n",
    "    \"\"\"\n",
    "    with open(input_file_path, \"r\", encoding=\"utf-8\") as infile, open(output_file_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        for line in infile:\n",
    "            # Detecting negative dates and converting them to string format\n",
    "            if '^^<http://www.w3.org/2001/XMLSchema#date>' in line:\n",
    "                start_index = line.find('\"') + 1\n",
    "                end_index = line.find('\"', start_index)\n",
    "                \n",
    "                if start_index != -1 and end_index != -1:\n",
    "                    date_string = line[start_index:end_index]\n",
    "                    if date_string.startswith(\"-\"):\n",
    "                        # Convert negative date to BCE string and remove type declaration\n",
    "                        sanitized_date = f'\"{date_string[1:]} BCE\"'\n",
    "                        # Remove the xsd:date type and keep it as a plain string\n",
    "                        line = line[:start_index-1] + sanitized_date + \" .\\n\"  # Added newline character\n",
    "            # Ensure each line ends with a newline character even if unmodified\n",
    "            if not line.endswith(\"\\n\"):\n",
    "                line += \"\\n\"\n",
    "            outfile.write(line)\n",
    "    \n",
    "    print(f\"Preprocessed RDF data saved to: {output_file_path}\")\n",
    "\n",
    "# Preprocess both datasets\n",
    "datasets_to_preprocess = [\n",
    "    (\"fr_en/en_att_triples\", \"fr_en/en_att_triples_preprocessed\"),\n",
    "    (\"fr_en/fr_att_triples\", \"fr_en/fr_att_triples_preprocessed\")\n",
    "]\n",
    "\n",
    "for input_file, output_file in datasets_to_preprocess:\n",
    "    preprocess_dates_in_file(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph creation & description generation\n",
    "Processes the datasets with the following steps:\n",
    "1) Creates a graph comprised of relation and attribute triples;\n",
    "2) Extracts all pieces of information about a node and stores it in a `.txt` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph\n",
    "import os\n",
    "\n",
    "def create_merged_graph(relation_file_path, attribute_file_path):\n",
    "    \"\"\"Creates and returns an RDF graph by merging relation and attribute files.\"\"\"\n",
    "    graph = Graph()\n",
    "    graph.parse(relation_file_path, format=\"nt\")\n",
    "    graph.parse(attribute_file_path, format=\"nt\")\n",
    "\n",
    "    print(f\"Merged graph created with {len(graph)} triples.\")\n",
    "    return graph\n",
    "\n",
    "def format_triples_for_embedding(graph, entity_uri, language_prefix):\n",
    "    \"\"\"\n",
    "    Formats RDF triples where the given entity is a subject or object into a readable format for embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    - graph: RDFLib Graph object\n",
    "    - entity_uri: URI of the entity to query for\n",
    "    - language_prefix: Prefix to indicate language (e.g., \"FR-\" or \"EN-\")\n",
    "\n",
    "    Returns:\n",
    "    - formatted_text: A single string containing all triples where the entity is subject or object, ready for embedding generation.\n",
    "    \"\"\"\n",
    "\n",
    "    def safe_split(uri):\n",
    "        \"\"\"Returns the language-prefixed label of a URI.\"\"\"\n",
    "        if uri is None:\n",
    "            return f\"{language_prefix}{entity_uri.split('/')[-1]}\"\n",
    "        return f\"{language_prefix}{uri.split('/')[-1]}\"\n",
    "\n",
    "    # Prepare SPARQL queries for both subject and object positions\n",
    "    query_subject = f\"\"\"\n",
    "    SELECT ?s ?p ?o\n",
    "    WHERE {{\n",
    "        <{entity_uri}> ?p ?o .\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    query_object = f\"\"\"\n",
    "    SELECT ?s ?p ?o\n",
    "    WHERE {{\n",
    "        ?s ?p <{entity_uri}> .\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute the queries\n",
    "    subject_results = graph.query(query_subject)\n",
    "    object_results = graph.query(query_object)\n",
    "\n",
    "    # Prepare the formatted text for embeddings\n",
    "    formatted_text = []\n",
    "\n",
    "    # Format triples where the entity is the subject\n",
    "    formatted_text.append(f\"# Triples where '{safe_split(entity_uri)}' is the subject:\\n\")\n",
    "    for s, p, o in subject_results:\n",
    "        formatted_text.append(f\"{safe_split(s)} {safe_split(p)} {safe_split(o)}.\")\n",
    "\n",
    "    # Format triples where the entity is the object\n",
    "    formatted_text.append(f\"\\n# Triples where '{safe_split(entity_uri)}' is the object:\\n\")\n",
    "    for s, p, o in object_results:\n",
    "        formatted_text.append(f\"{safe_split(s)} {safe_split(p)} {safe_split(o)}.\")\n",
    "\n",
    "    # Combine all triples into a single formatted string\n",
    "    formatted_text = \"\\n\".join(formatted_text)\n",
    "    return formatted_text\n",
    "\n",
    "def describe_node_for_embedding_per_subject(graph, output_file_prefix, language_prefix):\n",
    "    \"\"\"\n",
    "    Extracts all attributes and relations for every node and saves them in ten separate text files\n",
    "    (splitting the entities into ten equal parts for easier visualization).\n",
    "    \"\"\"\n",
    "    subjects = list(set(graph.subjects()))\n",
    "    total_subjects = len(subjects)\n",
    "    chunk_size = total_subjects // 10\n",
    "\n",
    "    os.makedirs(output_file_prefix, exist_ok=True)\n",
    "\n",
    "    # Split into ten files for easier management\n",
    "    for i in range(10):\n",
    "        start_index = i * chunk_size\n",
    "        end_index = (start_index + chunk_size) if (i < 9) else total_subjects\n",
    "        output_file = os.path.join(output_file_prefix, f\"part{i+1}.txt\")\n",
    "        \n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "            for subject in subjects[start_index:end_index]:\n",
    "                # Use the function to generate formatted triples for the subject\n",
    "                formatted_text = format_triples_for_embedding(graph, subject, language_prefix)\n",
    "                # Write the formatted data to the file with a separator for each entity\n",
    "                outfile.write(f\"\\n{'='*80}\\nEntity: {language_prefix}{subject.split('/')[-1]}\\n{'='*80}\\n\")\n",
    "                outfile.write(formatted_text + \"\\n\")\n",
    "        print(f\"File saved: '{output_file}'.\")\n",
    "\n",
    "def process_multiple_datasets(datasets):\n",
    "    \"\"\"Processes multiple RDF datasets and saves results for each in the specified folder.\"\"\"\n",
    "    for relation_file, attribute_file, output_folder, language_prefix in datasets:\n",
    "        graph = create_merged_graph(relation_file, attribute_file)\n",
    "        describe_node_for_embedding_per_subject(graph, output_folder, language_prefix)\n",
    "\n",
    "# List of datasets including both relation and attribute triples with the specified output folder and language prefix\n",
    "datasets_to_process = [\n",
    "    (\"fr_en/en_rel_triples_preprocessed\", \"fr_en/en_att_triples_preprocessed\", \"fr_en/en_combined\", \"EN-\"),\n",
    "    (\"fr_en/fr_rel_triples_preprocessed\", \"fr_en/fr_att_triples_preprocessed\", \"fr_en/fr_combined\", \"FR-\")\n",
    "]\n",
    "\n",
    "# Run batch processing\n",
    "process_multiple_datasets(datasets_to_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Pinecone index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from pinecone import Pinecone, ServerlessSpec \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set your API keys for Pinecone\n",
    "pc = Pinecone(\n",
    "    api_key=os.environ['PINECONE_API_KEY']\n",
    ")\n",
    "\n",
    "# Create Index if not already created\n",
    "pinecone_index_name = \"dl-proj-4\"\n",
    "if pinecone_index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=pinecone_index_name, \n",
    "        dimension=1536, # '1536' is the dimension for text-embedding-3-small\n",
    "        metric='cosine',\n",
    "        spec=ServerlessSpec(\n",
    "            cloud='aws',\n",
    "            region='us-east-1'\n",
    "        )\n",
    "    )\n",
    "     \n",
    "    while not pc.describe_index(pinecone_index_name).index.status['ready']:\n",
    "        time.sleep(1)\n",
    "    \n",
    "    print(\"Pinecone Index provisioned\")\n",
    "else:\n",
    "    print(\"Pinecone Index Already Provisioned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English dataset conversion to embeddings\n",
    "Create and insert embeddings for the english dataset (~ 19 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set your API keys for OpenAI\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "# Initialize OpenAI Embeddings using LangChain\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")  # Specify which embedding model\n",
    "\n",
    "# Load all text files from a directory\n",
    "directory_path = \"fr_en/en_combined\"  # directory path with all the national weather service documents\n",
    "loader = DirectoryLoader(directory_path, glob=\"*.txt\", loader_cls=TextLoader)  # Load only .txt files\n",
    "documents = loader.load()\n",
    "\n",
    "# Use a TextSplitter to split the documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "split_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "# Connect to the Pinecone index using LangChain's Pinecone wrapper\n",
    "# Add all the split documents into the Pinecone vector database\n",
    "pinecone_index_name = \"dl-proj-4\"\n",
    "vectorstore = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings)\n",
    "vectorstore.add_documents(documents=split_documents )\n",
    "\n",
    "print(\"English embeddings created and inserted in Pinecone Vector Database successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### French dataset conversion to embeddings\n",
    "Create and insert embeddings for the french dataset (~ 18 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set your API keys for OpenAI\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "# Initialize OpenAI Embeddings using LangChain\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")  # Specify which embedding model\n",
    "\n",
    "# Load all text files from a directory\n",
    "directory_path = \"fr_en/fr_combined\" \n",
    "loader = DirectoryLoader(directory_path, glob=\"*.txt\", loader_cls=TextLoader)  # Load only .txt files\n",
    "documents = loader.load()\n",
    "\n",
    "# Use a TextSplitter to split the documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "split_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "# Connect to the Pinecone index using LangChain's Pinecone wrapper\n",
    "# Add all the split documents into the Pinecone vector database\n",
    "pinecone_index_name = \"dl-proj-4\"\n",
    "vectorstore = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings)\n",
    "vectorstore.add_documents(documents=split_documents )\n",
    "\n",
    "print(\"French embeddings created and inserted in Pinecone Vector Database successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating predictions\n",
    "Uses the `gpt-4o-mini` pre-trained Large Language Model through OpenAI's API. The model receives the context retrieved from the Pinecone index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset\n",
    "Splits the dataset into 10 smaller parts for easier processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(file_path, output_folder=\"fr_en/split_parts\", num_parts=10):\n",
    "    \"\"\"Split a large dataset into smaller parts for batch processing with multiple API keys.\n",
    "    \n",
    "    Args:\n",
    "    - file_path (str): Path to the dataset file.\n",
    "    - output_folder (str): Folder to save the split parts.\n",
    "    - num_parts (int): Number of parts to split the dataset into.\n",
    "    \"\"\"\n",
    "    import os\n",
    "\n",
    "    # Ensure the output directory exists and is empty\n",
    "    if os.path.exists(output_folder):\n",
    "        # Clear the folder before saving new parts\n",
    "        for file in os.listdir(output_folder):\n",
    "            os.remove(os.path.join(output_folder, file))\n",
    "    else:\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Read all lines from the dataset\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Calculate the number of lines per part\n",
    "    total_lines = len(lines)\n",
    "    lines_per_part = total_lines // num_parts\n",
    "    print(f\"Total lines: {total_lines}, Lines per part: {lines_per_part}\")\n",
    "\n",
    "    # Split the dataset into multiple parts\n",
    "    for i in range(num_parts):\n",
    "        start_idx = i * lines_per_part\n",
    "        # For the last file, include all remaining lines\n",
    "        end_idx = start_idx + lines_per_part if i < num_parts - 1 else total_lines\n",
    "        part_lines = lines[start_idx:end_idx]\n",
    "\n",
    "        # Save each part as a separate file in the specified folder\n",
    "        part_file_path = os.path.join(output_folder, f\"ent_ILLs_part_{i + 1}.txt\")\n",
    "        with open(part_file_path, \"w\", encoding=\"utf-8\") as part_file:\n",
    "            part_file.writelines(part_lines)\n",
    "        print(f\"Part {i + 1} saved to '{part_file_path}' with {len(part_lines)} lines.\")\n",
    "\n",
    "# Usage Example\n",
    "split_dataset(\"fr_en/ent_ILLs.txt\", num_parts=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying the model\n",
    "Uses 5 API keys to avoid hitting the rate limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import tqdm\n",
    "import time\n",
    "import threading\n",
    "import glob\n",
    "\n",
    "# Load environment variables for multiple API keys\n",
    "load_dotenv()\n",
    "API_KEYS = [\n",
    "    os.environ.get('OPENAI_API_KEY_01'),\n",
    "    os.environ.get('OPENAI_API_KEY_02'),\n",
    "    os.environ.get('OPENAI_API_KEY_03'),\n",
    "    os.environ.get('OPENAI_API_KEY_04'),\n",
    "    os.environ.get('OPENAI_API_KEY_05')\n",
    "]\n",
    "\n",
    "# Initialize token buckets for rate and daily limits\n",
    "RATE_LIMIT = 500  # per minute per API key\n",
    "DAILY_LIMIT = 10000  # per day per API key\n",
    "api_key_index = 0\n",
    "rate_limit_buckets = [threading.Semaphore(RATE_LIMIT) for _ in API_KEYS]\n",
    "daily_token_buckets = [threading.Semaphore(DAILY_LIMIT) for _ in API_KEYS]\n",
    "lock = threading.Lock()  # For thread-safe key rotation\n",
    "\n",
    "def refill_tokens():\n",
    "    \"\"\"Refill the rate limit buckets every minute.\"\"\"\n",
    "    while True:\n",
    "        time.sleep(60)\n",
    "        for bucket in rate_limit_buckets:\n",
    "            for _ in range(RATE_LIMIT):\n",
    "                bucket.release()\n",
    "\n",
    "def reset_daily_tokens():\n",
    "    \"\"\"Reset the daily token buckets at midnight.\"\"\"\n",
    "    while True:\n",
    "        now = time.localtime()\n",
    "        seconds_until_midnight = (24 * 3600) - (now.tm_hour * 3600 + now.tm_min * 60 + now.tm_sec)\n",
    "        time.sleep(seconds_until_midnight)\n",
    "        for bucket in daily_token_buckets:\n",
    "            for _ in range(DAILY_LIMIT):\n",
    "                bucket.release()\n",
    "\n",
    "# Start the token refill threads\n",
    "threading.Thread(target=refill_tokens, daemon=True).start()\n",
    "threading.Thread(target=reset_daily_tokens, daemon=True).start()\n",
    "\n",
    "def get_next_api_key():\n",
    "    \"\"\"Rotate between API keys with thread safety.\"\"\"\n",
    "    global api_key_index\n",
    "    with lock:\n",
    "        api_key_index = (api_key_index + 1) % len(API_KEYS)\n",
    "    return API_KEYS[api_key_index], rate_limit_buckets[api_key_index], daily_token_buckets[api_key_index]\n",
    "\n",
    "def initialize_langchain(api_key):\n",
    "    \"\"\"Initialize LangChain components with a specific API key.\"\"\"\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", openai_api_key=api_key)\n",
    "    pinecone_index_name = \"dl-proj-4\"\n",
    "    vector_store = PineconeVectorStore(index_name=pinecone_index_name, embedding=embeddings)\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", openai_api_key=api_key, temperature=0.4)\n",
    "\n",
    "    prompt_template = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        Use the following context to identify the most similar entity in the French dataset (prefixed with FR-) for the given entity in the English dataset (prefixed with EN-):\n",
    "        Context: {context}\n",
    "        Given Entity: {question}\n",
    "        Provide only the name of the most similar entity from the dataset, prefixed with 'FR-'. Use the entity name, the provided context and your knowledge to identify the best answer.\n",
    "        Answer:\"\"\",\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "    llm_chain = prompt_template | llm | StrOutputParser()\n",
    "    return retriever, llm_chain\n",
    "\n",
    "def process_file(file_path):\n",
    "    \"\"\"Read the input file and return the modified entities for both EN and FR.\"\"\"\n",
    "    en_entities = []\n",
    "    fr_entities = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) == 2:\n",
    "                _, uri = parts\n",
    "                modified_uri = uri.replace(\"http://dbpedia.org/resource/\", \"\")\n",
    "                if uri.startswith(\"http://dbpedia.org/resource/FR-\"):\n",
    "                    fr_entities.append(f\"FR-{modified_uri}\")\n",
    "                else:\n",
    "                    en_entities.append(f\"EN-{modified_uri}\")\n",
    "    return en_entities, fr_entities\n",
    "\n",
    "def process_single_entity(entity, retries=3, delay=5):\n",
    "    \"\"\"Process a single entity with retries on failure.\"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            # API Key Handling\n",
    "            api_key, rate_bucket, daily_bucket = get_next_api_key()\n",
    "            rate_bucket.acquire()\n",
    "            daily_bucket.acquire()\n",
    "            \n",
    "            # Initialize LangChain Components\n",
    "            retriever, llm_chain = initialize_langchain(api_key)\n",
    "            \n",
    "            # Query Preparation\n",
    "            query = f\"{entity}\"\n",
    "            docs = retriever.invoke(query)\n",
    "            context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "            # Query the API\n",
    "            answer = llm_chain.invoke({\"context\": context, \"question\": entity})\n",
    "            if not answer: \n",
    "                print(f\"Empty response for {entity}\")\n",
    "                continue  # Retry if the answer is empty\n",
    "\n",
    "            # Prepare Result URIs\n",
    "            english_uri = f\"http://dbpedia.org/resource/{entity.replace('EN-', '')}\"\n",
    "            french_uri = f\"http://fr.dbpedia.org/resource/{answer.replace('FR-', '').replace(' ', '_')}\"\n",
    "            return english_uri, french_uri\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing entity {entity} (attempt {attempt + 1}/{retries}): {e}\")\n",
    "            time.sleep(delay)  # Wait before retrying\n",
    "\n",
    "    # If all retries fail, return None and log it\n",
    "    print(f\"Failed to process {entity} after {retries} attempts.\")\n",
    "    return None, None\n",
    "\n",
    "def save_failed_entities(failed_entities, output_file):\n",
    "    \"\"\"Save the entities that failed processing to a file for later reprocessing.\"\"\"\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        for entity in failed_entities:\n",
    "            file.write(f\"{entity}\\n\")\n",
    "    print(f\"\\nFailed entities saved to {output_file}\")\n",
    "\n",
    "# In the parallel processing function, collect failed entities\n",
    "def query_llm_for_entity_pairing_parallel(en_entities, max_workers=10):\n",
    "    results = {}\n",
    "    failed_entities = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(process_single_entity, entity): entity for entity in en_entities}\n",
    "\n",
    "        for future in tqdm.tqdm(as_completed(futures), total=len(futures)):\n",
    "            try:\n",
    "                english_uri, french_uri = future.result()\n",
    "                if english_uri and french_uri:\n",
    "                    results[english_uri] = french_uri\n",
    "                else:\n",
    "                    failed_entities.append(futures[future])\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                failed_entities.append(futures[future])\n",
    "\n",
    "    # Save failed entities for retry later\n",
    "    if failed_entities:\n",
    "        save_failed_entities(failed_entities, \"fr_en/failed_entities.txt\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def save_results_to_txt(results, output_file):\n",
    "    \"\"\"Save the alignment results to a .txt file.\"\"\"\n",
    "    # Ensure the directory exists before saving\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        for en_uri, fr_uri in results.items():\n",
    "            file.write(f\"{fr_uri}\\t{en_uri}\\n\")\n",
    "    print(f\"\\nResults saved to {output_file}\")\n",
    "\n",
    "# Process All Split Files\n",
    "split_files = sorted(glob.glob(\"fr_en/split_parts/ent_ILLs_part*.txt\"))\n",
    "\n",
    "for split_file in split_files:\n",
    "    output_file = split_file.replace(\"split_parts\", \"aligned_results\").replace(\".txt\", \"_aligned.txt\")\n",
    "    \n",
    "    # Skip if already processed\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"{split_file} already processed. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nProcessing: {split_file}\")\n",
    "    \n",
    "    # Load the current part\n",
    "    en_entities, fr_entities = process_file(split_file)\n",
    "\n",
    "    # Perform parallel processing\n",
    "    results = query_llm_for_entity_pairing_parallel(en_entities, max_workers=10)\n",
    "\n",
    "    # Save results immediately after processing each file\n",
    "    save_results_to_txt(results, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity comparison\n",
    "Compares the predictions generated by the LLM with the ground truth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction merging\n",
    "Combines the predictions generated by the model into a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the directories and output file path\n",
    "input_directory = \"fr_en/aligned_results/\"\n",
    "output_file = \"fr_en/aligned_entities.txt\"\n",
    "\n",
    "# Open the output file in write mode\n",
    "with open(output_file, \"w\") as outfile:\n",
    "    for i in range(1, 11):  # Loop from 1 to 10\n",
    "        filename = f\"ent_ILLs_part_{i}_aligned.txt\"\n",
    "        filepath = os.path.join(input_directory, filename)\n",
    "        \n",
    "        # Check if the file exists before merging\n",
    "        if os.path.exists(filepath):\n",
    "            print(f\"Merging {filename}\")\n",
    "            with open(filepath, \"r\") as infile:\n",
    "                outfile.write(infile.read())\n",
    "                outfile.write(\"\\n\")  # Add a newline between merged contents\n",
    "        else:\n",
    "            print(f\"File {filename} not found!\")\n",
    "\n",
    "print(f\"Merging complete. Results saved to {output_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hits@1\n",
    "Computes the hits@1 score and saves non-aligned entities inside a `.txt` file. Both the normalized and original URIs are saved inside the `fr_en` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "# Standardize all abbreviations to the same form\n",
    "ABBREVIATION_MAP = {\n",
    "    \"f.c.\": \"football_club\",\n",
    "    \"fc\": \"football_club\",\n",
    "    \"football club\": \"football_club\",\n",
    "    \"univ.\": \"university\",\n",
    "    \"univ\": \"university\",\n",
    "    \"co.\": \"company\",\n",
    "    \"co\": \"company\",\n",
    "    \"corp.\": \"corporation\",\n",
    "    \"corp\": \"corporation\"\n",
    "}\n",
    "\n",
    "def normalize_uri(uri):\n",
    "    \"\"\"\n",
    "    Normalize a URI by:\n",
    "    - Removing parentheses (but keeping the content inside)\n",
    "    - Trimming whitespace\n",
    "    - Lowercasing\n",
    "    - Removing accents from characters\n",
    "    - Standardizing both '-' and '_' to '_'\n",
    "    - Replacing all abbreviations with consistent forms\n",
    "    - Sorting words **only after the base URL**\n",
    "    \"\"\"\n",
    "    original_uri = uri.strip()\n",
    "    uri = uri.strip().lower()\n",
    "    \n",
    "    # Remove parentheses but keep their content\n",
    "    uri = uri.replace(\"(\", \"\").replace(\")\", \"\")\n",
    "\n",
    "    # Convert accented characters to base form (removes accents)\n",
    "    uri = unicodedata.normalize(\"NFD\", uri)\n",
    "    uri = \"\".join(char for char in uri if unicodedata.category(char) != 'Mn')  # Remove accents\n",
    "\n",
    "    # Replace all abbreviations with consistent forms\n",
    "    for abbr, full_form in ABBREVIATION_MAP.items():\n",
    "        uri = uri.replace(abbr, full_form)\n",
    "\n",
    "    # Normalize separators: Convert both '-' and '_' into a single '_'\n",
    "    uri = re.sub(r\"[-_]+\", \"_\", uri)  # Merge consecutive separators into one\n",
    "\n",
    "    # Split base URL and resource part for separate handling\n",
    "    if \"/resource/\" in uri:\n",
    "        base_url, entity = uri.split(\"/resource/\", 1)\n",
    "        # Sort the words **only for the entity part**\n",
    "        words = entity.split(\"_\")\n",
    "        words_sorted = sorted(words)\n",
    "        entity_normalized = \"_\".join(words_sorted).strip(\"_\")\n",
    "        uri = f\"{base_url}/resource/{entity_normalized}\"\n",
    "    else:\n",
    "        # If no base URL detected, normalize the whole string\n",
    "        words = uri.split(\"_\")\n",
    "        words_sorted = sorted(words)\n",
    "        uri = \"_\".join(words_sorted).strip(\"_\")\n",
    "    \n",
    "    return uri, original_uri  # Return both normalized and original versions\n",
    "\n",
    "def read_alignment_file(file_path):\n",
    "    \"\"\"\n",
    "    Read and normalize the alignment results file.\n",
    "    \"\"\"\n",
    "    alignment_results = {}\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) == 2:\n",
    "                fr_uri, en_uri = parts\n",
    "                # Normalize both URIs and store original versions too\n",
    "                fr_uri_normalized, fr_uri_original = normalize_uri(fr_uri)\n",
    "                en_uri_normalized, en_uri_original = normalize_uri(en_uri)\n",
    "                alignment_results[en_uri_normalized] = (fr_uri_normalized, fr_uri_original, en_uri_original)\n",
    "            else:\n",
    "                print(f\"Skipping invalid line: {line.strip()}\")\n",
    "    return alignment_results\n",
    "\n",
    "def read_ground_truth_file(file_path):\n",
    "    \"\"\"\n",
    "    Read and normalize the ground truth file.\n",
    "    \"\"\"\n",
    "    ground_truth = {}\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) == 2:\n",
    "                fr_uri, en_uri = parts\n",
    "                # Normalize both URIs and store original versions too\n",
    "                fr_uri_normalized, fr_uri_original = normalize_uri(fr_uri)\n",
    "                en_uri_normalized, en_uri_original = normalize_uri(en_uri)\n",
    "                ground_truth[en_uri_normalized] = (fr_uri_normalized, fr_uri_original, en_uri_original)\n",
    "            else:\n",
    "                print(f\"Skipping invalid line: {line.strip()}\")\n",
    "    return ground_truth\n",
    "\n",
    "def compute_hits_at_1_and_save_misses(alignment_results, ground_truth):\n",
    "    \"\"\"\n",
    "    Compute Hits@1 and save non-aligned entities to separate files:\n",
    "    - One for normalized results\n",
    "    - One for original results\n",
    "    \"\"\"\n",
    "    hits = 0\n",
    "    compared_entities = 0\n",
    "    non_aligned_normalized = []\n",
    "    non_aligned_original = []\n",
    "\n",
    "    # Adding headers for both files\n",
    "    non_aligned_normalized.append(\"English URI (normalized)\\tGround Truth French URI (normalized)\\tPredicted French URI (normalized)\")\n",
    "    non_aligned_original.append(\"English URI (original)\\tGround Truth French URI (original)\\tPredicted French URI (original)\")\n",
    "\n",
    "    for en_entity, (actual_fr_entity_norm, actual_fr_entity_orig, en_entity_orig) in ground_truth.items():\n",
    "        if en_entity in alignment_results:  # Only compare if the entity exists in both sets\n",
    "            compared_entities += 1\n",
    "            predicted_fr_entity_norm, predicted_fr_entity_orig, predicted_en_entity_orig = alignment_results[en_entity]\n",
    "            if predicted_fr_entity_norm == actual_fr_entity_norm:\n",
    "                hits += 1\n",
    "            else:\n",
    "                non_aligned_normalized.append(f\"{en_entity}\\t{actual_fr_entity_norm}\\t{predicted_fr_entity_norm or 'Not Found'}\")\n",
    "                non_aligned_original.append(f\"{en_entity_orig}\\t{actual_fr_entity_orig}\\t{predicted_fr_entity_orig or 'Not Found'}\")\n",
    "        else:\n",
    "            non_aligned_normalized.append(f\"{en_entity}\\t{actual_fr_entity_norm}\\tNot Found\")\n",
    "            non_aligned_original.append(f\"{en_entity_orig}\\t{actual_fr_entity_orig}\\tNot Found\")\n",
    "\n",
    "    # Save normalized results\n",
    "    normalized_file = \"fr_en/non_aligned_entities_normalized.txt\"\n",
    "    with open(normalized_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(\"\\n\".join(non_aligned_normalized))\n",
    "    \n",
    "    # Save original results\n",
    "    original_file = \"fr_en/non_aligned_entities_original.txt\"\n",
    "    with open(original_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(\"\\n\".join(non_aligned_original))\n",
    "    \n",
    "    # Print Hits@1 Score\n",
    "    print(f\"\\nHits@1 Score: {hits / compared_entities:.4f}\")\n",
    "    print(f\"Non-aligned normalized entities saved to: {normalized_file}\")\n",
    "    print(f\"Non-aligned original entities saved to: {original_file}\")\n",
    "    return hits / compared_entities\n",
    "\n",
    "# Running the entire process with strict normalization\n",
    "alignment_results_file = \"fr_en/aligned_entities.txt\"\n",
    "ground_truth_file = \"fr_en/ent_ILLs.txt\"\n",
    "\n",
    "alignment_results = read_alignment_file(alignment_results_file)\n",
    "ground_truth = read_ground_truth_file(ground_truth_file)\n",
    "hits_at_1_score = compute_hits_at_1_and_save_misses(alignment_results, ground_truth)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ea-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
